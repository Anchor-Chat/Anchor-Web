{"version":3,"sources":["webpack:///./node_modules/orbit-db-store/src/Loader.js","webpack:///./node_modules/orbit-db-store/src/Index.js","webpack:///./node_modules/ipfs-log/src/utils/difference.js","webpack:///./node_modules/ipfs-log/src/utils/intersection.js","webpack:///./node_modules/p-whilst/index.js","webpack:///./node_modules/ipfs-log/src/entry-io.js","webpack:///./node_modules/ipfs-log/src/log-io.js","webpack:///./node_modules/ipfs-log/src/g-set.js","webpack:///./node_modules/p-reduce/index.js","webpack:///./node_modules/p-each-series/index.js","webpack:///./node_modules/orbit-db-store/src/Store.js","webpack:///./node_modules/logplease/src/index.js","webpack:///./node_modules/p-map/index.js","webpack:///./node_modules/ipfs-log/src/utils/is-defined.js","webpack:///./node_modules/ipfs-log/src/lamport-clock.js","webpack:///./node_modules/ipfs-log/src/entry.js","webpack:///./node_modules/ipfs-log/src/utils/uniques.js","webpack:///./node_modules/ipfs-log/src/log-errors.js","webpack:///./node_modules/ipfs-log/src/log.js"],"names":["EventEmitter","__webpack_require__","pMap","Log","Logger","logger","create","color","Colors","Cyan","setLogLevel","getNext","e","next","flatMap","res","val","concat","notNull","entry","undefined","uniqueValues","batchSize","module","exports","[object Object]","store","concurrency","super","this","_store","_fetching","_stats","tasksRequested","tasksStarted","tasksProcessed","a","b","c","d","_buffer","_concurrency","_queue","_q","Set","_flushTimer","setInterval","tasksRunning","Object","keys","length","error","tasksFinished","setTimeout","_processQueue","tasksQueued","values","entries","notKnown","_oplog","has","hash","filter","forEach","_addToQueue","bind","console","capacity","items","slice","flattenAndGetUniques","nexts","reduce","processValues","logs","emit","load","_processOne","then","log","fromEntryHash","_ipfs","id","key","access","write","push","latest","_id","map","_index","oplog","processed","existing","isInFirst","hasBeenProcessed","wrap","fn","Promise","resolve","condition","action","loop","pWhilst","Entry","EntryIO","ipfs","hashes","exclude","timeout","onProgressCallback","concatArrays","arr1","arr2","fetchAll","Math","max","arr","amount","result","cache","loadingQueue","Array","isArray","addToLoadingQueue","shift","reject","timer","warn","_tasksRequested","fromMultihash","clearTimeout","isEntry","_tasksProcessed","catch","err","Clock","LogError","isDefined","_uniques","intersection","difference","last","n","immutabledb","ImmutableDBNotDefinedError","LogNotDefinedError","Error","object","put","toBuffer","dagNode","toJSON","multihash","get","enc","JSON","parse","data","logData","heads","NotALogError","clock","time","finalEntries","sort","compare","includes","entryHash","IpfsNotDefinedError","excludeHashes","fetchParallel","json","sourceEntries","combined","uniques","sliced","missingSourceEntries","withEntries","merge","tailHashes","size","missingOldEntries","withoutOldEntries","entryIntersection","value","set","iterable","reducer","initVal","iterator","Symbol","i","total","el","done","all","pReduce","Readable","mapSeries","Index","Loader","Blue","DefaultOptions","maxHistory","path","replicate","referenceCount","replicationConcurrency","address","options","opts","assign","_type","dbname","events","_cache","_keystore","keystore","_key","getKey","createKey","defaultAccess","admin","getPublic","read","accessController","_replicationInfo","buffered","queued","progress","snapshot","bytesLoaded","syncRequestsReceieved","_loader","on","apply","toString","have","bufferedLength","onLoadCompleted","async","join","updateIndex","type","removeAllListeners","close","destroy","localHeads","remoteHeads","head","_onLoadProgress","debug","logEntry","Buffer","from","stringify","dagObj","saved","unfinished","getQueue","snapshotData","toSnapshot","header","rs","Uint16Array","bytes","buffer","str","stream","content","files","add","queue","sync","catReadableStream","loadSnapshotData","buf","q","_byteSize","toArrayBuffer","ab","ArrayBuffer","view","Uint8Array","headerSize","parseInt","s","onProgress","count","fromJSON","batchOperation","lastOperation","append","fs","format","isNodejs","process","version","LogLevels","DEBUG","INFO","WARN","ERROR","NONE","GlobalLogLevel","GlobalLogfile","GlobalEvents","Black","Red","Green","Yellow","Magenta","Grey","White","Default","loglevelColors","defaultOptions","useColors","showTimestamp","showLevel","filename","appendFile","level","setLogfile","category","_shouldLog","_write","arguments","text","fileWriter","openSync","_format","unformattedText","_createLogMessage","formattedText","timestamp","writeSync","timestampFormat","levelFormat","categoryFormat","textFormat","levelColor","f","indexOf","categoryColor","Date","toISOString","envLogLevel","env","LOG","toUpperCase","logLevel","window","levels","forceBrowserMode","force","mapper","Infinity","TypeError","ret","isRejected","iterableDone","resolvingCount","currentIdx","nextItem","arg","LamportClock","dist","signKey","payload","v","signEntry","toMultihash","signature","sign","sig","pubKey","importPublicKey","verify","obj","distance","entry1","entry2","stack","parent","find","isParent","prev","GSet","LogIO","randomId","getTime","getHash","acc","getNextPointers","maxClockTimeReducer","uniqueEntriesReducer","_storage","_keys","_entryIndex","findHeads","_headsIndex","_nextsIndex","_length","maxTime","_clock","tails","findTails","findTailHashes","rootEntries","traversed","addToStack","rootEntry","pointerCount","newTime","traverse","isLog","verifyEntries","verifyEntry","every","newItems","pushToStack","tmp","nextsFromNewItems","mergedHeads","maxClock","payloadMapper","reverse","idx","len","findChildren","padding","fill","fromEntry","expandFrom","expand","reverseIndex","nullIndex","splice"],"mappings":"8EAAA,MAAAA,EAAAC,EAAA,KAAAD,aACAE,EAAAD,EAAA,KACAE,EAAAF,EAAA,KAEAG,EAAAH,EAAA,KACAI,EAAAD,EAAAE,OAAA,uBAAqDC,MAAAH,EAAAI,OAAAC,OACrDL,EAAAM,YAAA,SAEA,MACAC,EAAAC,KAAAC,KACAC,EAAA,CAAAC,EAAAC,IAAAD,EAAAE,OAAAD,GACAE,EAAAC,GAAA,OAAAA,QAAAC,IAAAD,EACAE,EAAA,CAAAN,EAAAC,KACAD,EAAAC,KACAD,GAGAO,EAAA,EA0LAC,EAAAC,sBAxLAxB,EACAyB,YAAAC,EAAAC,GACAC,QACAC,KAAAC,OAAAJ,EACAG,KAAAE,aACAF,KAAAG,QACAC,eAAA,EACAC,aAAA,EACAC,eAAA,EACAC,EAAA,EACAC,EAAA,EACAC,EAAA,EACAC,EAAA,GAEAV,KAAAW,WAEAX,KAAAY,aAAAd,GAAA,IACAE,KAAAa,UACAb,KAAAc,GAAA,IAAAC,IAGAf,KAAAgB,YAAAC,YAAA,KACA,IAAAjB,KAAAkB,cAAAC,OAAAC,KAAApB,KAAAa,QAAAQ,OAAA,IACA7C,EAAA8C,MAAA,0BAAAH,OAAAC,KAAApB,KAAAa,QAAAQ,OAAA,uBAAArB,KAAAI,eAAAJ,KAAAuB,cAAA,6BACAC,WAAA,IAAAxB,KAAAyB,gBAAA,KAEK,KAOLrB,qBACA,OAAAJ,KAAAG,OAAAC,eAOAC,mBACA,OAAAL,KAAAG,OAAAE,aAOAa,mBACA,OAAAlB,KAAAG,OAAAE,aAAAL,KAAAG,OAAAG,eAOAoB,kBACA,OAAAP,OAAAC,KAAApB,KAAAa,QAAAQ,OAAArB,KAAAkB,aAOAK,oBACA,OAAAvB,KAAAG,OAAAG,eAOAV,WACA,OAAAuB,OAAAQ,OAAA3B,KAAAa,QAMAjB,KAAAgC,GACA5B,KAAAG,OAAAI,GAAA,EACA,MAAAsB,EAAAvC,IAAAU,KAAAC,OAAA6B,OAAAC,IAAAzC,EAAA0C,MAAA1C,KAAAU,KAAAa,OAAAvB,EAAA0C,MAAA1C,GAEA,IACAsC,EACAK,OAAA5C,GACA4C,OAAAJ,GACAK,QAAAlC,KAAAmC,YAAAC,KAAApC,OAEAwB,WAAA,IAAAxB,KAAAyB,gBAAA,GACK,MAAA1C,GACLsD,QAAAf,MAAAvC,GAEAiB,KAAAG,OAAAI,IAGAX,YAAAN,GACAU,KAAAG,OAAAK,IACA,MAAAwB,EAAA1C,EAAA0C,MAAA1C,EAEAU,KAAAC,OAAA6B,OAAAC,IAAAC,IAAAhC,KAAAE,UAAA8B,IAAAhC,KAAAa,OAAAmB,GACAhC,KAAAG,OAAAK,KAIAR,KAAAG,OAAAC,gBAAA,EACAJ,KAAAa,OAAAmB,GAAA1C,EACAU,KAAAG,OAAAK,KAIAZ,sBAEA,GADAI,KAAAG,OAAAM,IACAT,KAAAkB,aAAAlB,KAAAY,aAAA,CACA,MAAA0B,EAAAtC,KAAAY,aAAAZ,KAAAkB,aAEAqB,EAAApB,OAAAQ,OAAA3B,KAAAa,QAAA2B,MAAA,EAAAF,GAAAL,OAAA5C,GACAkD,EAAAL,QAAA5C,UAAAU,KAAAa,OAAAvB,EAAA0C,MAAA1C,IAEA,MAAAmD,EAAAC,KAAAC,OAAA1D,MAAA0D,OAAAnD,MACAoD,EAAAF,IACA,MAAAf,EAAAR,OAAAQ,OAAAe,GAAAT,OAAA5C,GAGA,GAAAkD,EAAAlB,OAAA,GAAArB,KAAAW,QAAAU,OAAA,GACA,IAAArB,KAAAkB,cAAAlB,KAAAW,QAAAU,OAAA,GACA,MAAAwB,EAAA7C,KAAAW,QAAA6B,QACAxC,KAAAW,WAEAX,KAAA8C,KAAA,WAAAD,GAGAlB,EAAAN,OAAA,GACArB,KAAA+C,KAAApB,GAEA3B,KAAAG,OAAAM,KAGA,OAAApC,EAAAkE,EAAAxD,GAAAiB,KAAAgD,YAAAjE,IACAkE,KAAAR,GACAQ,KAAAL,IAIAhD,kBAAAN,GACAU,KAAAG,OAAAO,IACA,MAAAsB,EAAA1C,EAAA0C,MAAA1C,EAEA,GAAAU,KAAAC,OAAA6B,OAAAC,IAAAC,IAAAhC,KAAAE,UAAA8B,GAEA,YADAhC,KAAAG,OAAAO,IAIAV,KAAAE,UAAA8B,KACAhC,KAAA8C,KAAA,aAAAxD,GACAU,KAAAG,OAAAE,cAAA,EAIA,MACA6C,QAAA5E,EAAA6E,cAAAnD,KAAAC,OAAAmD,MAAApB,EAAAhC,KAAAC,OAAA6B,OAAAuB,GAAA5D,KAAAO,KAAAC,OAAAqD,IAAAtD,KAAAC,OAAAsD,OAAAC,OACAxD,KAAAW,QAAA8C,KAAAP,GAEA,MAAAQ,EAAAR,EAAAvB,OAAA,GAgBA,cAdA3B,KAAAa,OAAAmB,GAKAhC,KAAAG,OAAAG,gBAAA,EAKAN,KAAA8C,KAAA,gBAAA9C,KAAA2D,IAAA3B,EAAA0B,EAAA,KAAA1D,KAAAW,QAAAU,QAEArB,KAAAG,OAAAO,IAEAwC,EAAAvB,OAAAiC,IAAA9E,GAAA6D,OAAA1D,2CCjJAS,EAAAC,cAvBAC,YAAAyD,GACArD,KAAAqD,KACArD,KAAA6D,UAOAjE,MACA,OAAAI,KAAA6D,OAQAjE,YAAAkE,EAAAlC,GACA5B,KAAA6D,OAAAC,EAAAnC,4CCzBAjC,EAAAC,QAvBA,SAAAY,EAAAC,EAAA8C,GAEA,IAAAS,KACAC,KAiBA,OAbAzD,EAAA2B,QADAnD,GAAAiF,EAAAV,EAAAvE,EAAAuE,GAAAvE,IAAA,GAcAyB,EAAAmC,OAVA,CAAAzD,EAAAI,KACA,IAAA2E,OAAA1E,IAAAyE,EAAAV,EAAAhE,EAAAgE,GAAAhE,GACA4E,OAAA3E,IAAAwE,EAAAT,EAAAhE,EAAAgE,GAAAhE,GAKA,OAJA2E,GAAAC,IACAhF,EAAAuE,KAAAnE,GACAyE,EAAAT,EAAAhE,EAAAgE,GAAAhE,IAAA,GAEAJ,2CCMAQ,EAAAC,QAvBA,SAAAY,EAAAC,EAAA8C,GAEA,IAAAS,KACAC,KAiBA,OAbAzD,EAAA2B,QADAnD,GAAAiF,EAAAV,EAAAvE,EAAAuE,GAAAvE,IAAA,GAcAyB,EAAAmC,OAVA,CAAAzD,EAAAI,KACA,IAAA2E,OAAA1E,IAAAyE,EAAAV,EAAAhE,EAAAgE,GAAAhE,GACA4E,OAAA3E,IAAAwE,EAAAT,EAAAhE,EAAAgE,GAAAhE,GAKA,OAJA2E,IAAAC,IACAhF,EAAAuE,KAAAnE,GACAyE,EAAAT,EAAAhE,EAAAgE,GAAAhE,IAAA,GAEAJ,2CCjBA,MAAAiF,EAAAC,GAAA,IAAAC,QAAAC,IACAA,EAAAF,OAGA1E,EAAAC,QAAA,EAAA4E,EAAAC,IAAAL,EAAA,SAAAM,IACA,GAAAF,IACA,OAAAJ,EAAAK,GAAAvB,KAAAwB,yCCNA,MAAAC,EAAAtG,EAAA,MACAC,EAAAD,EAAA,KACAuG,EAAAvG,EAAA,WAKAwG,EAEAhF,qBAAAiF,EAAAC,EAAAzD,EAAA0D,KAAAjF,EAAAkF,EAAAC,GACA,MACAC,EAAA,CAAAC,EAAAC,IAAAD,EAAA/F,OAAAgG,GAEA,OAAA/G,EAAAyG,EAHA9C,GAAA4C,EAAAS,SAAAR,EAAA7C,EAAAX,EAAA0D,EAAAC,EAAAC,IAGmCnF,YAAAwF,KAAAC,IAAAzF,GAAAgF,EAAAzD,OAAA,KACnC4B,KAFAuC,KAAA7C,OAAAuC,OAiBAtF,gBAAAiF,EAAAC,EAAAW,EAAAV,KAAAC,EAAA,KAAAC,GACA,IAAAS,KACAC,KACAC,EAAAC,MAAAC,QAAAhB,GACAA,EAAAtC,SACAsC,GAGA,MAAAiB,EAAAhH,GAAA6G,EAAAnC,KAAA1E,GAIAgG,EAAA7C,QADAnD,GAAA4G,EAAA5G,EAAAiD,MAAAjD,GAkDA,OAAA2F,EA/CA,IACAkB,EAAAvE,OAAA,IACAqE,EAAArE,OAAAoE,KAAA,GAGA,KACA,MAAAzD,EAAA4D,EAAAI,QAEA,OAAAL,EAAA3D,GACAqC,QAAAC,UAGA,IAAAD,QAAA,CAAAC,EAAA2B,KAGA,MAAAC,EAAAlB,EACAxD,WAAA,KACAa,QAAA8D,uCAA2DnE,0BAA6BgD,QACxFV,KACWU,GACX,KAeAoB,EAGAzB,EAAA0B,cAAAxB,EAAA7C,GACAiB,KAjBA3D,IACAgH,aAAAJ,GACAvB,EAAA4B,QAAAjH,KACAA,EAAAN,KAAAkD,QAAA6D,GACAL,EAAAjC,KAAAnE,GACAqG,EAAA3D,GAAA1C,EACAkH,EACAvB,GACAA,EAAAjD,EAAA1C,EAAAoG,EAAArE,WAUA4B,KAAAqB,GACAmC,MAAAC,IACApC,UAMArB,KAAA,IAAAyC,IAIAhG,EAAAC,QAAAiF,qCC/FAxG,EAAA,WACAuG,EAAAvG,EAAA,KACAwG,EAAAxG,EAAA,MACAuI,EAAAvI,EAAA,KACAwI,EAAAxI,EAAA,KACAyI,EAAAzI,EAAA,KACA0I,EAAA1I,EAAA,KACA2I,EAAA3I,EAAA,MACA4I,EAAA5I,EAAA,MAEA6I,EAAA,CAAAzB,EAAA0B,IAAA1B,EAAAhD,MAAAgD,EAAAnE,OAAA6F,EAAA1B,EAAAnE,QAiPA3B,EAAAC,cA9OAC,mBAAAuH,EAAAjE,GACA,IAAA2D,EAAAM,GAAA,MAAAP,EAAAQ,6BACA,IAAAP,EAAA3D,GAAA,MAAA0D,EAAAS,qBAEA,GAAAnE,EAAAvB,OAAAN,OAAA,YAAAiG,MAAA,gCAEA,OAAAH,EAAAI,OAAAC,IAAAtE,EAAAuE,YACAxE,KAAAyE,KAAAC,SAAAC,WAWAhI,qBAAAuH,EAAAnF,EAAAX,GAAA,EAAA0D,EAAAE,GACA,IAAA4B,EAAAM,GAAA,MAAAP,EAAAQ,6BACA,IAAAP,EAAA7E,GAAA,UAAAsF,uBAA2DtF,KAE3D,OAAAmF,EAAAI,OAAAM,IAAA7F,GAAyC8F,IAAA,WACzC7E,KAAAyE,GAAAK,KAAAC,MAAAN,EAAAC,SAAAM,OAEAhF,KAAAiF,IACA,IAAAA,EAAAC,QAAAD,EAAA7E,GAAA,MAAAuD,EAAAwB,eACA,OAAAxD,EAAAS,SAAA8B,EAAAe,EAAAC,MAAA9G,EAAA0D,EAAA,KAAAE,GACAhC,KAAArB,IAEA,MAAAyG,EAAAzG,EAAAe,OAAA,CAAA0F,EAAA/I,IACAA,EAAA+I,MAAAC,KAAAD,EAAAC,KACA,IAAA3B,EAAArH,EAAA+I,MAAAhF,GAAA/D,EAAA+I,MAAAC,MAEAD,EACa,IAAA1B,EAAAuB,EAAA7E,KACbkF,EAAA3G,EAAAY,QAAAgG,KAAA7D,EAAA8D,SACAN,EAAAI,EAAAtG,OAAAlD,GAAAmJ,EAAAC,MAAAO,SAAA3J,EAAAiD,OACA,OACAqB,GAAA6E,EAAA7E,GACA1B,OAAA4G,EACAJ,QACAE,aAMAzI,qBAAAiF,EAAA8D,EAAAtF,EAAAhC,GAAA,EAAA0D,EAAAE,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAgC,sBACA,IAAA/B,EAAA8B,GAAA,UAAArB,MAAA,+BAGAjG,KAAA,EAAAiE,KAAAC,IAAAlE,EAAA,GAAAA,EAGA,MAAAwH,EAAA9D,EAEA,OAAAH,EAAAkE,cAAAjE,GAAA8D,GAAAtH,EAAAwH,EAAA,UAAA5D,GACAhC,KAAArB,KAKAD,OAFAN,GAAA,EAAA4F,EAAArF,EAAAP,GAAAO,KAOAhC,gBAAAiF,EAAAkE,EAAA1H,GAAA,EAAAiC,EAAA0B,EAAAC,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAQ,6BAUA,OAAAxC,EAAAkE,cAAAjE,EAAAkE,EAAAZ,MAAAvE,IAAA7E,KAAAiD,MAAAX,KAAA,GAAA2D,EAAAC,GACAhC,KAAArB,IACA,MAAA2G,EAAA3G,EAAAY,QAAAgG,KAAA7D,EAAA8D,SAEA,OADA7G,EAAAK,OAAAlD,GAAAgK,EAAAZ,MAAAO,SAAA3J,EAAAiD,QAEAqB,GAAA0F,EAAA1F,GACA1B,OAAA4G,EACAJ,MAAAY,EAAAZ,SAcAvI,iBAAAuH,EAAA6B,EAAA3H,GAAA,EAAA0D,EAAAzB,EAAAlC,EAAA6D,GACA,IAAA4B,EAAAM,GAAA,MAAAP,EAAAQ,6BACA,IAAAP,EAAAmC,GAAA,UAAA1B,MAAA,mCAGA,IAAAzB,MAAAC,QAAAkD,KAAArE,EAAA4B,QAAAyC,GACA,UAAA1B,MAAA,kFAGAzB,MAAAC,QAAAkD,KACAA,OAIA3H,KAAA,EAAAiE,KAAAC,IAAAlE,EAAA2H,EAAA3H,UAGA,MAAAwH,EAAA9D,IAAAnB,IAAA7E,KAAAiD,KAAAjD,EAAAiD,KAAAjD,GAAAgG,EACAD,EAAAkE,EAAApF,IAAA7E,KAAAiD,MAEA,OAAA4C,EAAAkE,cAAA3B,EAAArC,EAAAzD,EAAAwH,EAAA,UAAA5D,GACAhC,KAAArB,IACA,IAAAqH,EAAAD,EAAA5J,OAAAwC,GACAsH,EAAApC,EAAAmC,EAAA,QAAAT,KAAA7D,EAAA8D,SAGA,MAAAU,EAAAD,EAAA1G,MAAAnB,GAAA,GAAAA,GAAA6H,EAAA7H,QAIA+H,EAAApC,EAAAmC,EAAAH,EAAA,QASAtD,GAPA2D,EAOAD,EANAD,GADA5I,EAOA4I,GANA3G,MAAA6G,EAAAhI,OAAAd,EAAAc,QACAgI,EAAAjK,OAAA+J,IAMA,OACA9F,GAAAqC,IAAArE,OAAA,GAAAgC,GACA1B,OAAA+D,KAVA,IAAAnF,EAAA8I,EACAF,EAuBAvJ,kBAAAiF,EAAA3B,EAAAtB,EAAA6D,GAAA,GACA,IAAAoB,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA3D,GAAA,MAAA0D,EAAAS,qBACA,IAAAR,EAAAjF,GAAA,UAAA0F,MAAA,uCAEAzB,MAAAC,QAAAlE,KACAA,OAKA,MAAAkD,EAAAlD,EAAAgC,IAAA7E,KAAAC,MAAAiD,OAAAlD,KAAAsC,OAAA,GAGA,WAAAyD,EAAAzD,OACAgD,QAAAC,SAA8B3C,OAAAuB,EAAAvB,SAG9BiD,EAAAkE,cAAAjE,EAAAC,EAAAW,EAAAvC,EAAAvB,QACAsB,KAAArB,KAMAD,OAHAuB,EAAAoG,MAAA1H,EAAAY,MAAA,EAAAiD,OAQA7F,cAAAiF,EAAA3B,EAAAuC,GAAA,GACA,IAAAoB,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA3D,GAAA,MAAA0D,EAAAS,qBAGA,WAAAnE,EAAAqG,WAAAlI,OACAgD,QAAAC,SAA8B3C,OAAAuB,EAAAvB,SAG9BiD,EAAAkE,cAAAjE,EAAA3B,EAAAqG,WAAA9D,EAAAvC,EAAAvB,QACAsB,KAAArB,IAEA,MAAA4H,EAAA/D,GAAA,EAAAvC,EAAAvB,OAAAN,OAAAoE,GAAA,EAGAwD,EAAA/F,EAAAvB,OAAAvC,OAAAwC,GAAA4G,KAAA7D,EAAA8D,SACAU,EAAAK,GAAA,EAAAP,EAAAzG,OAAAgH,GAAAP,EAAAzG,QAUAiH,EAAAzC,EAAAmC,EAAAjG,EAAAvB,OAAA,QAAA6G,KAAA7D,EAAA8D,SACAiB,EAAA1C,EAAA9D,EAAAvB,OAAAwH,EAAA,QAAAX,KAAA7D,EAAA8D,SACAkB,EAAA5C,EAAA7D,EAAAvB,OAAAwH,EAAA,QAAAX,KAAA7D,EAAA8D,SAGApH,EAAAmI,GAAAG,EAAAtI,OAAAoI,EAAApI,QAIAiI,EAAA,CAAA/I,EAAAC,KACA,IAAAyI,EAGA,OAFAA,EAAA1I,EAAAnB,OAAAoB,GACAsG,EAAAmC,EAAA,QACAT,KAAA7D,EAAA8D,UAIA,OACA9G,OAFA2H,EAAAG,EAAAH,EAAAK,EAVAtI,GAAA,EAAAqI,EAAAlH,OAAAnB,GAAAqI,4CCvNAhK,EAAAC,cATAC,WAAA+B,IACA/B,OAAAgK,IACAhK,MAAAiK,IACAjK,IAAAgK,IACAhK,IAAAgK,IACAjI,cACAN,mDCfA3B,EAAAC,QAAA,EAAAmK,EAAAC,EAAAC,IAAA,IAAA3F,QAAA,CAAAC,EAAA2B,KACA,MAAAgE,EAAAH,EAAAI,OAAAD,YACA,IAAAE,EAAA,EAEA,MAAAnL,EAAAoL,IACA,MAAAC,EAAAJ,EAAAjL,OAEAqL,EAAAC,KACAhG,EAAA8F,GAIA/F,QAAAkG,KAAAH,EAAAC,EAAAT,QACA3G,KAAA2G,IACA5K,EAAA+K,EAAAH,EAAA,GAAAA,EAAA,GAAAO,QAEA1D,MAAAR,IAGAjH,EAAAgL,yCCnBA,MAAAQ,EAAApM,EAAA,MAEAsB,EAAAC,QAAA,EAAAmK,EAAAG,IAAAO,EAAAV,EAAA,CAAAvJ,EAAAC,EAAA2J,IAAAF,EAAAzJ,EAAA2J,IAAAlH,KAAA,IAAA6G,mDCDA1L,EAAA,WACAD,EAAAC,EAAA,KAAAD,aACAsM,EAAArM,EAAA,KACAsM,EAAAtM,EAAA,MACAE,EAAAF,EAAA,KACAuM,EAAAvM,EAAA,MACAwM,EAAAxM,EAAA,MAEAG,EAAAH,EAAA,KACAI,EAAAD,EAAAE,OAAA,kBAAgDC,MAAAH,EAAAI,OAAAkM,OAChDtM,EAAAM,YAAA,SAEA,MAAAiM,GACAH,QACAI,YAAA,EACAC,KAAA,YACAC,WAAA,EACAC,eAAA,GACAC,uBAAA,KAsZAzL,EAAAC,cAlZAC,YAAAiF,EAAAxB,EAAA+H,EAAAC,GAEA,IAAAC,EAAAnK,OAAAoK,UAA+BT,GAC/B3J,OAAAoK,OAAAD,EAAAD,GACArL,KAAAqL,QAAAC,EAGAtL,KAAAwL,MAAA,QAGAxL,KAAAqD,KACArD,KAAAoL,UACApL,KAAAyL,OAAAL,EAAAJ,MAAA,GACAhL,KAAA0L,OAAA,IAAAvN,EAGA6B,KAAAoD,MAAAyB,EACA7E,KAAA2L,OAAAN,EAAA1F,MACA3F,KAAA6D,OAAA,IAAA7D,KAAAqL,QAAAV,MAAA3K,KAAAqD,IAEArD,KAAA4L,UAAAP,EAAAQ,SACA7L,KAAA8L,KAAAT,KAAA/H,IACA+H,EAAA/H,IACAtD,KAAA4L,UAAAG,OAAA1I,IAAArD,KAAA4L,UAAAI,UAAA3I,GAEArD,KAAAoD,MAAAyI,SAAA7L,KAAA4L,UAGA,MAAAK,GACAC,OAAAlM,KAAA8L,KAAAK,UAAA,QACAC,QACA5I,OAAAxD,KAAA8L,KAAAK,UAAA,SAEAnM,KAAAuD,OAAA8H,EAAAgB,kBAAAJ,EAGAjM,KAAA8B,OAAA,IAAAxD,EAAA0B,KAAAoD,MAAApD,KAAAqD,GAAA,eAAArD,KAAA8L,KAAA9L,KAAAuD,OAAAC,OAGAxD,KAAAsM,kBACAC,SAAA,EACAC,OAAA,EACAC,SAAA,EACAlH,IAAA,GAIAvF,KAAAG,QACAuM,UACAC,aAAA,GAEAC,sBAAA,GAGA,IACA5M,KAAA6M,QAAA,IAAAjC,EAAA5K,UAAAqL,QAAAF,wBACAnL,KAAA6M,QAAAC,GAAA,aAAAxN,IAEAU,KAAAsM,iBAAAE,SACAxM,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAA/G,IAAAvF,KAAA8B,OAAAT,OAAA/B,EAAA+I,MAAA/I,EAAA+I,MAAAC,KAAA,IAEAtI,KAAA0L,OAAA5I,KAAA,YAAA9C,KAAAoL,QAAA4B,WAAA1N,KAEAU,KAAA6M,QAAAC,GAAA,iBAAAzJ,EAAArB,EAAA1C,EAAA2N,EAAAC,KAEAlN,KAAAsM,iBAAAC,SAAAW,EACAlN,KAAAsM,iBAAAG,SAAAzM,KAAAsM,iBAAAG,SAAAS,EAEAlN,KAAAsM,iBAAAG,SAAAnH,KAAAC,IAAAwH,MAAA,MAAA/M,KAAA8B,OAAAT,OAAArB,KAAAsM,iBAAAG,SAAAzM,KAAA8B,OAAAT,OAAA6L,IAGAlN,KAAAsM,iBAAAC,SAAAW,EACAlN,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAA/G,IAAAvF,KAAAsM,iBAAAG,WAEAzM,KAAA0L,OAAA5I,KAAA,qBAAA9C,KAAAoL,QAAA4B,WAAAhL,EAAA1C,EAAAU,KAAAsM,iBAAAG,SAAAQ,KAGA,MAAAE,EAAAC,MAAAvK,EAAAoK,KACA,IACA,QAAA/J,KAAAL,QACA7C,KAAA8B,OAAAuL,KAAAnK,GAAA,EAAAlD,KAAA8B,OAAAuB,IAEArD,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAvF,KAAAsM,iBAAA/G,IAAAvF,KAAA8B,OAAAT,QACArB,KAAA6D,OAAAyJ,YAAAtN,KAAA8B,QACA9B,KAAAsM,iBAAAG,SAAAnH,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAAG,SAAAzM,KAAA8B,OAAAT,SACArB,KAAAsM,iBAAAE,QAAA3J,EAAAxB,OAEArB,KAAA0L,OAAA5I,KAAA,aAAA9C,KAAAoL,QAAA4B,WAAAnK,EAAAxB,QACS,MAAAtC,GACTsD,QAAAf,MAAAvC,KAGAiB,KAAA6M,QAAAC,GAAA,WAAAK,GACK,MAAApO,GACLsD,QAAAf,MAAA,eAAAvC,IAIAwL,UACA,OAAA1E,MAAAC,QAAA9F,KAAA6D,eACA7D,KAAA6D,cACA1C,OAAAC,KAAApB,KAAA6D,eAAAD,IAAA7E,GAAAiB,KAAA6D,cAAA9E,IAGAwO,WACA,OAAAvN,KAAAwL,MAGAlI,UACA,OAAAtD,KAAA8L,KAGAlM,cA6BA,OA3BAI,KAAAsM,kBACAC,SAAA,EACAC,OAAA,EACAC,SAAA,EACAlH,IAAA,GAGAvF,KAAAG,QACAuM,UACAC,aAAA,GAEAC,sBAAA,GAGA5M,KAAA0L,OAAA8B,mBAAA,QACAxN,KAAA0L,OAAA8B,mBAAA,iBACAxN,KAAA0L,OAAA8B,mBAAA,aACAxN,KAAA0L,OAAA8B,mBAAA,sBACAxN,KAAA0L,OAAA8B,mBAAA,cACAxN,KAAA0L,OAAA8B,mBAAA,SACAxN,KAAA0L,OAAA8B,mBAAA,eAGAxN,KAAA2L,OAAA8B,QAGAzN,KAAA0L,OAAA5I,KAAA,SAAA9C,KAAAoL,QAAA4B,YACA3I,QAAAC,UAGA1E,mBACAI,KAAAyN,cACAzN,KAAA2L,OAAA+B,UACA1N,KAAA6D,OAAA,IAAA7D,KAAAqL,QAAAV,MAAA3K,KAAAqD,IACArD,KAAA8B,OAAA,IAAAxD,EAAA0B,KAAAoD,MAAApD,KAAAqD,GAAA,eAAArD,KAAA8L,KAAA9L,KAAAuD,OAAAC,OACAxD,KAAA2L,OAAA3L,KAAAqL,QAAA1F,MAGA/F,WAAA6F,GACAA,KAAAzF,KAAAqL,QAAAN,WAEA,MAAA4C,QAAA3N,KAAA2L,OAAA9D,IAAA,mBACA+F,QAAA5N,KAAA2L,OAAA9D,IAAA,oBACAM,EAAAwF,EAAAvO,OAAAwO,GAEAzF,EAAA9G,OAAA,GACArB,KAAA0L,OAAA5I,KAAA,OAAA9C,KAAAoL,QAAA4B,WAAA7E,SAEAuC,EAAAvC,EAAAiF,MAAAS,IACA7N,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAvF,KAAAsM,iBAAA/G,IAAAsI,EAAAxF,MAAAC,MACA,IAAApF,QAAA5E,EAAA6E,cAAAnD,KAAAoD,MAAAyK,EAAA7L,KAAAhC,KAAA8B,OAAAuB,GAAAoC,EAAAzF,KAAA8B,OAAAH,OAAA3B,KAAAsD,IAAAtD,KAAAuD,OAAAC,MAAAxD,KAAA8N,gBAAA1L,KAAApC,aACAA,KAAA8B,OAAAuL,KAAAnK,EAAAuC,EAAAzF,KAAA8B,OAAAuB,IACArD,KAAAsM,iBAAAG,SAAAnH,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAAG,SAAAzM,KAAA8B,OAAAT,SACArB,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAA/G,IAAAvF,KAAAsM,iBAAAG,aAIAtE,EAAA9G,OAAA,GACArB,KAAA6D,OAAAyJ,YAAAtN,KAAA8B,QAEA9B,KAAA0L,OAAA5I,KAAA,QAAA9C,KAAAoL,QAAA4B,WAAAhN,KAAA8B,OAAAqG,OAGAvI,KAAAuI,GAIA,GAHAnI,KAAAG,OAAAyM,uBAAA,EACApO,EAAAuP,uBAAkC/N,KAAAG,OAAAyM,yBAAqCzE,EAAA9G,UAEvE,IAAA8G,EAAA9G,OAwCA,OAAAqJ,EAAAvC,EA9BA0F,IACA,IAAAA,EAEA,OADAxL,QAAA8D,KAAA,0CACA9B,QAAAC,QAAA,MAGA,IAAAtE,KAAAuD,OAAAC,MAAAkF,SAAAmF,EAAAvK,OAAAtD,KAAAuD,OAAAC,MAAAkF,SAAA,KAEA,OADArG,QAAA8D,KAAA,8FACA9B,QAAAC,QAAA,MAKA,MAAA0J,EAAA7M,OAAAoK,UAAuCsC,GAEvC,OADAG,EAAAhM,KAAA,KACAhC,KAAAoD,MAAAmE,OAAAC,IAAAyG,EAAAC,KAAAnG,KAAAoG,UAAAH,KACA/K,KAAAmL,KAAAzG,SAAAC,WACA3E,KAAAjB,IAIAA,IAAA6L,EAAA7L,MACAK,QAAA8D,KAAA,kDAGAnE,IAEAiB,KAAA,IAAA4K,KAIA5K,KAAAmK,MAAAiB,UACArO,KAAA2L,OAAA9B,IAAA,eAAA1B,GACA3J,EAAAuP,qBAAoC5F,EAAA9G,WAAiBgN,EAAAzK,IAAA7E,KAAAiD,MAAAqL,KAAA,UACrDrN,KAAA6M,QAAA9J,KAAAsL,EAAApM,OAAAlD,GAAA,OAAAA,MAIAa,aAAA6F,EAAA7D,GACA5B,KAAA6M,QAAA9J,KAAAnB,GAGAhC,qBACA,MAAA0O,EAAAtO,KAAA6M,QAAA0B,WAEA,IAAAC,EAAAxO,KAAA8B,OAAA2M,aACAC,EAAA,IAAAT,EAAAlG,KAAAoG,WACA9K,GAAAmL,EAAAnL,GACA8E,MAAAqG,EAAArG,MACAqB,KAAAgF,EAAA7M,OAAAN,OACAkM,KAAAvN,KAAAuN,QAEA,MAAAoB,EAAA,IAAAlE,EACA,IAAAjB,EAAA,IAAAoF,aAAAF,EAAArN,SACAwN,EAAA,IAAAZ,EAAAzE,EAAAsF,QACAH,EAAAlL,KAAAoL,GACAF,EAAAlL,KAAAiL,GASAF,EAAA7M,OAAAO,QAPA/C,IACA,IAAA4P,EAAA,IAAAd,EAAAlG,KAAAoG,UAAAhP,IACAqK,EAAA,IAAAoF,aAAAG,EAAA1N,SACAsN,EAAAlL,KAAA,IAAAwK,EAAAzE,EAAAsF,SACAH,EAAAlL,KAAAsL,KAIAJ,EAAAlL,KAAA,MAEA,MAAAuL,GACAhE,KAAAhL,KAAAoL,QAAA4B,WACAiC,QAAAN,GAGAjC,QAAA1M,KAAAoD,MAAA8L,MAAAC,IAAAH,GAOA,aALAhP,KAAA2L,OAAA9B,IAAA,WAAA6C,IAAArL,OAAA,UACArB,KAAA2L,OAAA9B,IAAA,QAAAyE,GAEA9P,EAAAuP,yBAAoCrB,IAAArL,OAAA,GAAAW,uBAAqDsM,EAAAjN,UAEzFqL,EAGA9M,uBAAAqF,GACAjF,KAAA0L,OAAA5I,KAAA,OAAA9C,KAAAoL,QAAA4B,YAEA,MAAAoC,QAAApP,KAAA2L,OAAA9D,IAAA,SACA7H,KAAAqP,KAAAD,OAEA,MAAA1C,QAAA1M,KAAA2L,OAAA9D,IAAA,YAEA,IAAA6E,EA4FA,UAAApF,sBAAsCtH,KAAAoL,sBA5FtC,CACA,MAAAlM,QAAAc,KAAAoD,MAAA8L,MAAAI,kBAAA5C,EAAA1K,MACAuN,EAAA,IACA,IAAAlL,QAAA,CAAAC,EAAA2B,KACA,IAAAuJ,EAAA,IAAAvB,EAAA,GACAwB,KA+DAvQ,EAAA4N,GAAA,OA7DApM,IAEA,GADAV,KAAA0P,WAAAhP,EAAAW,OACAoO,EAAApO,OAAA,IACAoO,EAAAhM,KAAA/C,OACa,CACb,MAAAH,EAAA0N,EAAA7O,OAAAqQ,GACAD,EAAAvB,EAAA7O,QAAAoQ,EAAAjP,IACAkP,QAuDAvQ,EAAA4N,GAAA,MAnDA,KAGA,GAAA2C,EAAApO,OAAA,GACA,MAAAd,EAAA0N,EAAA7O,OAAAqQ,GACAD,EAAAvB,EAAA7O,QAAAoQ,EAAAjP,IAGA,SAAAoP,EAAAH,GAGA,IAFA,IAAAI,EAAA,IAAAC,YAAAL,EAAAnO,QACAyO,EAAA,IAAAC,WAAAH,GACAzF,EAAA,EAA6BA,EAAAqF,EAAAnO,SAAgB8I,EAC7C2F,EAAA3F,GAAAqF,EAAArF,GAEA,OAAAyF,EAGA,MAAAI,EAAAC,SAAA,IAAArB,YAAAe,EAAAH,EAAAhN,MAAA,QACA,IAAAkM,EAEA,IACAA,EAAA3G,KAAAC,MAAAwH,EAAAhN,MAAA,EAAAwN,EAAA,IACa,MAAAjR,IAIb,IAAA4C,KACApB,EAAA,EAAAyP,EACA,KAAAzP,EAAAiP,EAAAnO,QAAA,CACA,MAAA6O,EAAAD,SAAA,IAAArB,YAAAe,EAAAH,EAAAhN,MAAAjC,IAAA,MAEAA,GAAA,EACA,MAAA0H,EAAAuH,EAAAhN,MAAAjC,IAAA2P,GACA,IACA,MAAAxP,EAAAqH,KAAAC,MAAAC,GACAtG,EAAA8B,KAAA/C,GACe,MAAA3B,IAEfwB,GAAA2P,EAGAxB,GACA1O,KAAAwL,MAAAkD,EAAAnB,KACAvN,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAvF,KAAAsM,iBAAA/G,IAAA5D,EAAAgB,OAAA,CAAAzD,EAAAC,IAAAmG,KAAAC,IAAArG,EAAAC,EAAAkJ,MAAAC,MAAA,IACAhE,GAAuB3C,SAAA0B,GAAAqL,EAAArL,GAAA8E,MAAAuG,EAAAvG,MAAAoF,KAAAmB,EAAAnB,SAEvBvN,KAAAsM,iBAAA/G,IAAA,EACAjB,GAAuB3C,SAAA0B,GAAA,KAAA8E,MAAA,KAAAoF,KAAA,YAQvB4C,EAAA,CAAAnO,EAAA1C,EAAA8Q,EAAAhG,KACApK,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAvF,KAAAsM,iBAAA/G,IAAAjG,EAAA+I,MAAAC,MACAtI,KAAAsM,iBAAAG,SAAAnH,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAAG,SAAA2D,EAAApQ,KAAA8B,OAAAT,SACArB,KAAA8N,gBAAA9L,EAAA1C,EAAAU,KAAAsM,iBAAAG,SAAAzM,KAAAsM,iBAAA/G,MAKAiJ,QAAAe,IACA,GAAAf,EAAA,CACA,MAAAtL,QAAA5E,EAAA+R,SAAArQ,KAAAoD,MAAAoL,GAAA,EAAAxO,KAAA8L,KAAA9L,KAAAuD,OAAAC,MAAA,IAAA2M,SACAnQ,KAAA8B,OAAAuL,KAAAnK,GAAA,EAAAlD,KAAA8B,OAAAuB,IACArD,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAA/G,IAAAvF,KAAAsM,iBAAAG,SAAAzM,KAAA8B,OAAAT,SACArB,KAAAsM,iBAAAG,SAAAnH,KAAAC,IAAAvF,KAAAsM,iBAAAG,SAAAzM,KAAA8B,OAAAT,QACArB,KAAA6D,OAAAyJ,YAAAtN,KAAA8B,QACA9B,KAAA0L,OAAA5I,KAAA,aAAA9C,KAAAoL,QAAA4B,YAEAhN,KAAA0L,OAAA5I,KAAA,QAAA9C,KAAAoL,QAAA4B,WAAAhN,KAAA8B,OAAAqG,OAIA,OAAAnI,KAGAJ,oBAAAqI,EAAAqI,EAAAC,EAAAtL,GACA,GAAAjF,KAAA8B,OAAA,CACA,MAAAxC,QAAAU,KAAA8B,OAAA0O,OAAAvI,EAAAjI,KAAAqL,QAAAH,gBAQA,OAPAlL,KAAAsM,iBAAAG,WACAzM,KAAAsM,iBAAA/G,IAAAD,KAAAC,IAAAwH,MAAA,MAAA/M,KAAAsM,iBAAA/G,IAAAvF,KAAAsM,iBAAAG,SAAAnN,EAAA+I,MAAAC,OACAtI,KAAAoL,QAAA4B,iBACAhN,KAAA2L,OAAA9B,IAAA,eAAAvK,IACAU,KAAA6D,OAAAyJ,YAAAtN,KAAA8B,QACA9B,KAAA0L,OAAA5I,KAAA,QAAA9C,KAAAoL,QAAA4B,WAAA1N,EAAAU,KAAA8B,OAAAqG,OACAlD,KAAA3F,GACAA,EAAA0C,MAIApC,mBAAAqI,EAAAqI,EAAAC,EAAAtL,GACA,UAAAqC,MAAA,oBAGA1H,gBAAAoC,EAAA1C,EAAAmN,EAAArC,GACApK,KAAA0L,OAAA5I,KAAA,gBAAA9C,KAAAoL,QAAA4B,WAAAhL,EAAA1C,EAAAgG,KAAAC,IAAAvF,KAAA8B,OAAAT,OAAAoL,GAAAnH,KAAAC,IAAAvF,KAAA8B,OAAAT,QAAA,EAAArB,KAAAsM,iBAAA/G,KAAA,gFCpaA,MAAAkL,EAAArS,EAAA,KACAsS,EAAAtS,EAAA,KAAAsS,OACAvS,EAAAC,EAAA,KAAAD,aAEA,IAAAwS,IAAAC,EAAAC,QAEA,MAAAC,GACAC,MAAA,QACAC,KAAA,OACAC,KAAA,OACAC,MAAA,QACAC,KAAA,QAIA,IAAAC,EAAAN,EAAAC,MAGAM,EAAA,KAEAC,EAAA,IAAAnT,EAGAQ,GACA4S,MAAA,EACAC,IAAA,EACAC,MAAA,EACAC,OAAA,EACA7G,KAAA,EACA8G,QAAA,EACA/S,KAAA,EACAgT,KAAA,EACAC,MAAA,EACAC,QAAA,GAIAnB,IACAhS,GACA4S,MAAA,QACAC,IAAA,YACAC,MAAA,YACAC,OAAA,SACA7G,KAAA,YACA8G,QAAA,SACA/S,KAAA,UACAgT,KAAA,UACAC,MAAA,QACAC,QAAA,UAIA,MAAAC,GAAApT,EAAAC,KAAAD,EAAA8S,MAAA9S,EAAA+S,OAAA/S,EAAA6S,IAAA7S,EAAAmT,SAEAE,GACAC,WAAA,EACAvT,MAAAC,EAAAmT,QACAI,eAAA,EACAC,WAAA,EACAC,SAAAf,EACAgB,YAAA,GAgKA3S,EAAAC,SACAhB,SACAmS,YACAjS,YAAAyT,IACAlB,EAAAkB,GAEAC,WAAAH,IACAf,EAAAe,GAEA3T,OAAA,CAAA+T,EAAAnH,KAEA,OADA,UAtKAzL,YAAA4S,EAAAnH,GACArL,KAAAwS,WACA,IAAAlH,KACAnK,OAAAoK,OAAAD,EAAA0G,GACA7Q,OAAAoK,OAAAD,EAAAD,GACArL,KAAAqL,QAAAC,EAGA1L,QACAI,KAAAyS,WAAA3B,EAAAC,QACA/Q,KAAA0S,OAAA5B,EAAAC,MAAAL,EAAA3D,MAAA,KAAA4F,YAGA/S,MACAI,KAAAyS,WAAA3B,EAAAC,QACA/Q,KAAA+N,MAAAhB,MAAA/M,KAAA2S,WAGA/S,OACAI,KAAAyS,WAAA3B,EAAAE,OACAhR,KAAA0S,OAAA5B,EAAAE,KAAAN,EAAA3D,MAAA,KAAA4F,YAGA/S,OACAI,KAAAyS,WAAA3B,EAAAG,OACAjR,KAAA0S,OAAA5B,EAAAG,KAAAP,EAAA3D,MAAA,KAAA4F,YAGA/S,QACAI,KAAAyS,WAAA3B,EAAAI,QACAlR,KAAA0S,OAAA5B,EAAAI,MAAAR,EAAA3D,MAAA,KAAA4F,YAGA/S,OAAA0S,EAAAM,IACA5S,KAAAqL,QAAA+G,UAAAf,KAAArR,KAAA6S,YAAAlC,IACA3Q,KAAA6S,WAAApC,EAAAqC,SAAA9S,KAAAqL,QAAA+G,UAAAf,EAAArR,KAAAqL,QAAAgH,WAAA,YAEA,IAAA3B,EAAA1Q,KAAA+S,QAAAT,EAAAM,GACAI,EAAAhT,KAAAiT,kBAAAX,EAAAM,GACAM,EAAAlT,KAAAiT,kBAAAX,EAAAM,EAAAlC,EAAAyC,UAAAzC,EAAA4B,MAAA5B,EAAA8B,SAAA9B,EAAAkC,MAEA5S,KAAA6S,YAAAlC,GACAF,EAAA2C,UAAApT,KAAA6S,WAAAG,EAAA,mBAEArC,IAAA3Q,KAAAqL,QAAA4G,WACA5P,QAAAa,IAAAgQ,GACA5B,EAAAxO,KAAA,OAAA9C,KAAAwS,SAAAF,EAAAM,IAGAN,IAAAxB,EAAAI,MACAlR,KAAAqL,QAAA6G,eAAAlS,KAAAqL,QAAA8G,UACA9P,QAAAf,MAAA4R,EAAAxC,EAAAyC,UAAAzC,EAAA4B,MAAA5B,EAAA8B,SAAA9B,EAAAkC,MACS5S,KAAAqL,QAAA6G,gBAAAlS,KAAAqL,QAAA8G,UACT9P,QAAAf,MAAA4R,EAAAxC,EAAAyC,UAAAzC,EAAA8B,SAAA9B,EAAAkC,OACS5S,KAAAqL,QAAA6G,eAAAlS,KAAAqL,QAAA8G,UACT9P,QAAAf,MAAA4R,EAAAxC,EAAA4B,MAAA5B,EAAA8B,SAAA9B,EAAAkC,MAEAvQ,QAAAf,MAAA4R,EAAAxC,EAAA8B,SAAA9B,EAAAkC,MAGA5S,KAAAqL,QAAA6G,eAAAlS,KAAAqL,QAAA8G,UACA9P,QAAAa,IAAAgQ,EAAAxC,EAAAyC,UAAAzC,EAAA4B,MAAA5B,EAAA8B,SAAA9B,EAAAkC,MACS5S,KAAAqL,QAAA6G,gBAAAlS,KAAAqL,QAAA8G,UACT9P,QAAAa,IAAAgQ,EAAAxC,EAAAyC,UAAAzC,EAAA8B,SAAA9B,EAAAkC,OACS5S,KAAAqL,QAAA6G,eAAAlS,KAAAqL,QAAA8G,UACT9P,QAAAa,IAAAgQ,EAAAxC,EAAA4B,MAAA5B,EAAA8B,SAAA9B,EAAAkC,MAEAvQ,QAAAa,IAAAgQ,EAAAxC,EAAA8B,SAAA9B,EAAAkC,MAMAhT,QAAA0S,EAAAM,GACA,IAAAS,EAAA,GACAC,EAAA,GACAC,EAAA,GACAC,EAAA,KAEA,GAAAxT,KAAAqL,QAAA4G,UAAA,CACA,MAAAwB,EAAAtS,OAAAC,KAAA0P,GAAAlN,IAAA8P,GAAA5C,EAAA4C,IAAAC,QAAArB,GACAsB,EAAA5T,KAAAqL,QAAA3M,MAEAiS,GACA3Q,KAAAqL,QAAA6G,gBACAmB,EAAA,MAAA1U,EAAAiT,KAAA,KAEA5R,KAAAqL,QAAA8G,YACAmB,EAAA,MAAAvB,EAAA0B,GAAA,QAEAF,EAAA,MAAAK,EAAA,MACAJ,EAAA,WAEAxT,KAAAqL,QAAA6G,gBACAmB,EAAA,SAAA1U,EAAAiT,MAEA5R,KAAAqL,QAAA8G,YACAmB,EAAA,SAAAvB,EAAA0B,IAEAF,EAAA,SAAAK,EAAA,uBAIA,OACAT,UAAAE,EACAf,MAAAgB,EACAd,SAAAe,EACAX,KAAAY,GAIA5T,kBAAA0S,EAAAM,EAAAS,EAAAC,EAAAC,EAAAC,GACAH,KAAA,GACAC,KAAA,GACAC,KAAA,GACAC,KAAA,MAEA7C,GAAA3Q,KAAAqL,QAAA4G,YACAjS,KAAAqL,QAAA6G,gBACAmB,EAAA,MAEArT,KAAAqL,QAAA8G,YACAmB,EAAA,MAEAC,EAAA,KACAC,EAAA,QAGA,IAAA9N,EAAA,GAYA,OAVA1F,KAAAqL,QAAA6G,gBACAxM,IAAA,IAAAmO,MAAAC,cAAA,KAEApO,EAAA2N,EAAA3N,EAEA1F,KAAAqL,QAAA8G,YACAzM,GAAA4N,EAAA,IAAAhB,EAAA,KAAAA,IAAAxB,EAAAE,MAAAsB,IAAAxB,EAAAG,KAAA,aAEAvL,GAAA6N,EAAAvT,KAAAwS,SACA9M,GAAA8N,EAAAZ,EAIAhT,WAAA0S,GACA,IAAAyB,OAAA,IAAAnD,QAAArR,IAAAqR,EAAAoD,UAAAzU,IAAAqR,EAAAoD,IAAAC,IAAArD,EAAAoD,IAAAC,IAAAC,cAAA,KAGA,MAAAC,GAFAJ,EAAA,oBAAAK,eAAAH,IAAAG,OAAAH,IAAAC,cAAAH,IAEA3C,EACAiD,EAAAlT,OAAAC,KAAA0P,GAAAlN,IAAA8P,GAAA5C,EAAA4C,IAGA,OAFAW,EAAAV,QAAArB,IACA+B,EAAAV,QAAAQ,KAgBA3B,EAAAnH,IAGAiJ,iBAAAC,GAAA5D,GAAA4D,EACA7I,OAAA4F,yDC3OA5R,EAAAC,QAAA,EAAAmK,EAAA0K,EAAAlJ,IAAA,IAAAjH,QAAA,CAAAC,EAAA2B,KAKA,GAJAqF,EAAAnK,OAAAoK,QACAzL,YAAA2U,KACEnJ,GAEF,mBAAAkJ,EACA,UAAAE,UAAA,+BAGA,MAAA5U,EAAAwL,EAAAxL,YAEA,sBAAAA,MAAA,GACA,UAAA4U,0EAAsF5U,eAAkBA,MAGxG,MAAA6U,KACA1K,EAAAH,EAAAI,OAAAD,YACA,IAAA2K,GAAA,EACAC,GAAA,EACAC,EAAA,EACAC,EAAA,EAEA,MAAA/V,EAAA,KACA,GAAA4V,EACA,OAGA,MAAAI,EAAA/K,EAAAjL,OACAmL,EAAA4K,EAGA,GAFAA,IAEAC,EAAA1K,KAOA,OANAuK,GAAA,OAEA,IAAAC,GACAxQ,EAAAqQ,IAMAG,IAEAzQ,QAAAC,QAAA0Q,EAAApL,OACA3G,KAAAoH,GAAAmK,EAAAnK,EAAAF,IACAlH,KACA9D,IACAwV,EAAAxK,GAAAhL,EACA2V,IACA9V,KAEA0H,IACAkO,GAAA,EACA3O,EAAAS,MAKA,QAAAyD,EAAA,EAAgBA,EAAArK,IAChBd,KAEA6V,GAHiC1K,2CCvDjCzK,EAAAC,QAFA,CAAAsV,QAAA1V,IAAA0V,GAAA,OAAAA,2CCAAC,EACAtV,YAAAyD,EAAAiF,GACAtI,KAAAqD,KACArD,KAAAsI,QAAA,EAGA1I,OACA,WAAAsV,EAAAlV,KAAAqD,KAAArD,KAAAsI,MAGA1I,MAAAyI,GAEA,OADArI,KAAAsI,KAAAhD,KAAAC,IAAAvF,KAAAsI,KAAAD,EAAAC,MACA,IAAA4M,EAAAlV,KAAAqD,GAAArD,KAAAsI,MAGA1I,QACA,WAAAsV,EAAAlV,KAAAqD,GAAArD,KAAAsI,MAGA1I,eAAAW,EAAAC,GAEA,IAAA2U,EAAA5U,EAAA+H,KAAA9H,EAAA8H,KAIA,WAAA6M,GAAA5U,EAAA8C,KAAA7C,EAAA6C,GAAA9C,EAAA8C,GAAA7C,EAAA6C,IAAA,IAEA8R,GAIAzV,EAAAC,QAAAuV,iDC/BA,MAAAvO,EAAAvI,EAAA,KACAyI,EAAAzI,EAAA,KAEAwK,EAAA,QAAAtB,MAAA,mCAEA3C,EAYA/E,oBAAAiF,EAAAxB,EAAA4E,EAAAjJ,KAAAqJ,EAAA+M,GACA,IAAAvO,EAAAhC,GAAA,MAAA+D,IACA,IAAA/B,EAAAxD,GAAA,UAAAiE,MAAA,wBACA,IAAAT,EAAAoB,GAAA,UAAAX,MAAA,uBACA,IAAAT,EAAA7H,KAAA6G,MAAAC,QAAA9G,GAAA,UAAAsI,MAAA,mCAIA,IAGAhI,GACA0C,KAAA,KACAqB,KACAgS,QAAApN,EACAjJ,KAPAA,EAAAiD,OAAA4E,GACAjD,IAFA7E,KAAAiD,KAAAjD,EAAAiD,KAAAjD,GASAuW,EAAA,EACAjN,MAAA,IAAA1B,EAAAtD,EAAAgF,IAAAC,KAAA,OASA,OALAzD,EAAAgH,UAAAuJ,IACA9V,QAAAqF,EAAA4Q,UAAA1Q,EAAAgH,SAAAvM,EAAA8V,IAGA9V,EAAA0C,WAAA2C,EAAA6Q,YAAA3Q,EAAAvF,GACAA,EAGAM,uBAAAiM,EAAAvM,EAAAgE,GACA,MAAAmS,QAAA5J,EAAA6J,KAAApS,EAAA,IAAA2K,EAAAlG,KAAAoG,UAAA7O,KAGA,OAFAA,EAAAqW,IAAAF,EACAnW,EAAAgE,MAAA6I,UAAA,OACA7M,EAGAM,yBAAAN,EAAAuM,GACA,MAAA9M,EAAAoC,OAAAoK,WACAvJ,KAAA,KACAqB,GAAA/D,EAAA+D,GACAgS,QAAA/V,EAAA+V,QACArW,KAAAM,EAAAN,KACAsW,EAAAhW,EAAAgW,EACAjN,MAAA/I,EAAA+I,QAGAuN,QAAA/J,EAAAgK,gBAAAvW,EAAAgE,WACAuI,EAAAiK,OAAAxW,EAAAqW,IAAAC,EAAA,IAAA3H,EAAAlG,KAAAoG,UAAApP,KAaAa,mBAAAiF,EAAAvF,GACA,IAAAuF,EAAA,MAAA+D,IACA,MAAAX,EAAAgG,EAAAC,KAAAnG,KAAAoG,UAAA7O,IACA,OAAAuF,EAAA0C,OAAAC,IAAAS,GACAhF,KAAA/D,KAAAyI,SAAAC,WAaAhI,qBAAAiF,EAAA7C,GACA,IAAA6C,EAAA,MAAA+D,IACA,IAAA5G,EAAA,UAAAsF,uBAAgDtF,KAChD,OAAA6C,EAAA0C,OAAAM,IAAA7F,GAAkC8F,IAAA,WAClC7E,KAAA8S,GAAAhO,KAAAC,MAAA+N,EAAApO,SAAAM,OACAhF,KAAAgF,IACA,IAAA3I,GACA0C,OACAqB,GAAA4E,EAAA5E,GACAgS,QAAApN,EAAAoN,QACArW,KAAAiJ,EAAAjJ,KACAsW,EAAArN,EAAAqN,EACAjN,MAAAJ,EAAAI,OAIA,OAFAJ,EAAA0N,KAAAxU,OAAAoK,OAAAjM,GAA4CqW,IAAA1N,EAAA0N,MAC5C1N,EAAA3E,KAAAnC,OAAAoK,OAAAjM,GAA4CgE,IAAA2E,EAAA3E,MAC5ChE,IASAM,eAAAmW,GACA,YAAAxW,IAAAwW,EAAA1S,SACA9D,IAAAwW,EAAA/W,WACAO,IAAAwW,EAAA/T,WACAzC,IAAAwW,EAAAV,cACA9V,IAAAwW,EAAAT,QACA/V,IAAAwW,EAAA1N,MAGAzI,eAAAW,EAAAC,GACA,IAAAwV,EAAArP,EAAA8B,QAAAlI,EAAA8H,MAAA7H,EAAA6H,OACA,WAAA2N,EAAAzV,EAAA8H,MAAAhF,GAAA7C,EAAA6H,MAAAhF,IAAA,IACA2S,EASApW,eAAAW,EAAAC,GACA,OAAAD,EAAAyB,OAAAxB,EAAAwB,KASApC,gBAAAqW,EAAAC,GACA,OAAAA,EAAAlX,KAAA2U,QAAAsC,EAAAjU,OAAA,EAaApC,oBAAAN,EAAAqC,GAIA,IAHA,IAAAwU,KACAC,EAAAzU,EAAA0U,KAAAtX,GAAA4F,EAAA2R,SAAAhX,EAAAP,IACAwX,EAAAjX,EACA8W,GACAD,EAAA1S,KAAA2S,GACAG,EAAAH,EACAA,EAAAzU,EAAA0U,KAAAtX,GAAA4F,EAAA2R,SAAAC,EAAAxX,IAGA,OADAoX,IAAA3N,KAAA,CAAAjI,EAAAC,IAAAD,EAAA8H,MAAAC,KAAA/H,EAAA8H,MAAAC,OAKA5I,EAAAC,QAAAgF,+DCzKAjF,EAAAC,QATA,SAAAiK,EAAAtG,GAEA,IAAA4F,KAIA,OADAU,EAAA1H,QADAnD,GAAAmK,EAAA5F,EAAAvE,EAAAuE,GAAAvE,MAEAoC,OAAAC,KAAA8H,GAAAtF,IAHA7E,GAAAmK,EAAAnK,uCCCAW,EAAAC,SACAyH,2BALA,QAAAE,MAAA,oCAMAD,mBALA,QAAAC,MAAA,4BAMAc,aALA,QAAAd,MAAA,4FCFA,MAAAjJ,EAAAD,EAAA,KACAoY,EAAApY,EAAA,MACAuG,EAAAvG,EAAA,KACAqY,EAAArY,EAAA,MACAwI,EAAAxI,EAAA,KACAuI,EAAAvI,EAAA,KACAyI,EAAAzI,EAAA,KACA0I,EAAA1I,EAAA,KAEAsY,EAAA,SAAA7C,MAAA8C,UAAA3J,WACA4J,EAAA7X,KAAAiD,KACA/C,EAAA,CAAAC,EAAA2X,IAAA3X,EAAAE,OAAAyX,GACAC,EAAAxX,KAAAN,KACA+X,EAAA,CAAA7X,EAAA2X,IAAAvR,KAAAC,IAAArG,EAAA2X,EAAAxO,MAAAC,MACA0O,EAAA,CAAA9X,EAAA2X,KACA3X,EAAA2X,EAAA7U,MAAA6U,EACA3X,SAaAZ,UAAAkY,EAUA5W,YAAAiF,EAAAxB,EAAAzB,EAAAuG,EAAAE,EAAA/E,EAAAlC,MACA,IAAAyF,EAAAhC,GACA,MAAA+B,EAAAQ,6BAGA,GAAAP,EAAAjF,KAAAiE,MAAAC,QAAAlE,GACA,UAAA0F,MAAA,0DAGA,GAAAT,EAAAsB,KAAAtC,MAAAC,QAAAqC,GACA,UAAAb,MAAA,qCAGAvH,QAEAC,KAAAiX,SAAApS,EACA7E,KAAA2D,IAAAN,GAAAqT,IAGA1W,KAAA4L,UAAA5L,KAAAiX,SAAApL,SACA7L,KAAA8L,KAAAxI,EACAtD,KAAAkX,MAAArR,MAAAC,QAAA1E,SAGAQ,QACA5B,KAAAmX,YAAAvV,EAAAe,OAAAqU,MAGA7O,KAAA7J,EAAA8Y,UAAAxV,GACA5B,KAAAqX,YAAAlP,EAAAxF,OAAAqU,MAGAhX,KAAAsX,eACA1V,EAAAM,QAAAnD,KAAAC,KAAAkD,QAAA3B,GAAAP,KAAAsX,YAAA/W,GAAAxB,EAAAiD,OAGAhC,KAAAuX,QAAA3V,IAAAP,OAAA,EAGA,MAAAmW,EAAAlS,KAAAC,IAAA8C,IAAAC,KAAA,EAAAtI,KAAAmI,MAAAxF,OAAAoU,EAAA,IACA/W,KAAAyX,OAAA,IAAA9Q,EAAA3G,KAAAqD,GAAAmU,GAOAnU,SACA,OAAArD,KAAA2D,IAOA0E,YACA,OAAArI,KAAAyX,OAOApW,aACA,OAAArB,KAAAuX,QAOA5V,aACA,OAAAR,OAAAQ,OAAA3B,KAAAmX,aAAA3O,KAAA7D,EAAA8D,aAOAN,YACA,OAAAhH,OAAAQ,OAAA3B,KAAAqX,iBAQAK,YACA,OAAApZ,EAAAqZ,UAAA3X,KAAA2B,QAQA4H,iBACA,OAAAjL,EAAAsZ,eAAA5X,KAAA2B,QAQA/B,IAAAoC,GACA,OAAAhC,KAAAmX,YAAAnV,GAGApC,IAAAN,GACA,YAAAC,IAAAS,KAAAmX,YAAA7X,EAAA0C,MAAA1C,GAGAM,SAAAiY,EAAApS,GAEA,IAAA0Q,EAAA0B,EAAAjU,IAAAkT,GAAAnU,OAAA1D,MACA6Y,KACApS,KACA0K,EAAA,EAEA,MAAA2H,EAAA/V,IACA0D,EAAA1D,IAAA8V,EAAA9V,KACAmU,EAAA1S,KAAAzB,GACA8V,EAAA9V,IAAA,IAYA,IAFA6V,EAAA3V,QANA8V,IACAtS,EAAAsS,EAAAhW,MAAAgW,EAAAhW,KACA8V,EAAAE,EAAAhW,OAAA,EACAoO,MAKA+F,EAAA9U,OAAA,GAAA+O,EAAA3K,GAAA,CACA,MAAAzD,EAAAmU,EAAAnQ,QACA1G,EAAAU,KAAA6H,IAAA7F,GACA1C,IACA8Q,IACA1K,EAAApG,EAAA0C,MAAA1C,EAAA0C,KACA8V,EAAAxY,EAAA0C,OAAA,EACA1C,EAAAN,KAAAkD,QAAA6V,IAGA,OAAArS,EAQA9F,aAAAqI,EAAAgQ,EAAA,GAEA,GAAAjY,KAAA8L,OACA9L,KAAAkX,MAAAxO,SAAA1I,KAAA8L,KAAAK,UAAA,UACAnM,KAAAkX,MAAAxO,SAAA,KACA,UAAApB,MAAA,wBAIA,MAAA4Q,EAAA5S,KAAAC,IAAAvF,KAAAqI,MAAAC,KAAAtI,KAAAmI,MAAAxF,OAAAoU,EAAA,MACA/W,KAAAyX,OAAA,IAAA9Q,EAAA3G,KAAAqI,MAAAhF,GAAA6U,GAEA,MAAAxV,EAAAvB,OAAAC,KAAApB,KAAAmY,SAAAnY,KAAAmI,MAAA8P,IAEA3Y,QAAAqF,EAAAlG,OAAAuB,KAAAiX,SAAAjX,KAAAqD,GAAA4E,EAAAvF,EAAA1C,KAAAqI,MAAArI,KAAA8L,MAOA,OANA9L,KAAAmX,YAAA7X,EAAA0C,MAAA1C,EACAoD,EAAAR,QAAAnD,GAAAiB,KAAAsX,YAAAvY,GAAAO,EAAA0C,MACAhC,KAAAqX,eACArX,KAAAqX,YAAA/X,EAAA0C,MAAA1C,EAEAU,KAAAuX,UACAjY,EAkBAM,WAAAsD,EAAAsG,GAAA,EAAAnG,GACA,IAAAwD,EAAA3D,GAAA,MAAA0D,EAAAS,qBACA,IAAA/I,EAAA8Z,MAAAlV,GAAA,MAAA0D,EAAAwB,eAIA,MAAAiQ,EAAAjL,MAAAxL,IAOA5B,KAAAkX,MAAAtT,IALA7E,KAAAoN,UAAApN,EAAAoN,UAAA,OAAApN,GAmCA,aADAV,EAAAuD,EA3BAwL,MAAA9N,IACA,IAAAA,EAAAgE,IAAA,UAAAgE,MAAA,mCACA,IAAAhI,EAAAqW,IAAA,UAAArO,MAAA,kCAEA,OAAAtH,KAAAkX,MAAA7V,QAAArB,KAAAkX,MAAA,KAAAlX,KAAA8L,MACAxM,EAAA+D,KAAArD,KAAAqD,GACA,UAAAiE,MAAA,+CAGA,GAAAtH,KAAAkX,MAAA7V,OAAA,IACArB,KAAAkX,MAAAxO,SAAA,OAhBAtH,EAiBApB,KAAAkX,MAAA9X,QAAAY,KAAA8L,OAjBAxM,EAiBAA,GAfA8B,EAAAiV,KADAtX,OAAAO,EAAAgE,MAkBA,OADAjB,QAAA8D,KAAA,+FACA,EAGA,UACAxB,EAAA2T,YAAAhZ,EAAAU,KAAA4L,WACS,MAAA7M,GAGT,OAFAsD,QAAAa,IAAAnE,GACAsD,QAAAa,IAAA,2BAAA5D,IACA,EAGA,YAIAiZ,MApCAxZ,IAAA,IAAAA,IAEA,IAAAqC,EAAA9B,EA8DA+D,MAAAH,EAAAlD,MAAAwI,KAAA,CAAAjI,EAAAC,IAAAD,EAAA8C,GAAA7C,EAAA6C,IAAA,GAAAA,GAGA,MAAAmV,EA5BA,EAAAtV,EAAA6B,KACA,IAAAoR,EAAAhV,OAAAC,KAAA8B,EAAAmU,aACAS,KACA5Y,KAEA,MAAAuZ,EAAAzW,IACA8V,EAAA9V,IAAA+C,EAAA8C,IAAA7F,KACAmU,EAAA1S,KAAAzB,GACA8V,EAAA9V,IAAA,IAIA,KAAAmU,EAAA9U,OAAA,IACA,MAAAW,EAAAmU,EAAAnQ,QACA1G,EAAA4D,EAAA2E,IAAA7F,GACA1C,IAAAyF,EAAA8C,IAAA7F,KACA9C,EAAAI,EAAA0C,MAAA1C,EACAwY,EAAAxY,EAAA0C,OAAA,EACA1C,EAAAN,KAAAkD,QAAAuW,IAGA,OAAAvZ,GAOA8H,CAAA9D,EAAAlD,MAGA,GAAAA,KAAA8L,KAAA,CAGA,UAFAuM,EAAAlX,OAAAQ,OAAA6W,IAGA,OAAAxY,KAIAA,KAAAmX,YAAAhW,OAAAoK,OAAAvL,KAAAmX,YAAAqB,GAUA,GANArX,OAAAQ,OAAA6W,GAAAtW,QADAnD,KAAAC,KAAAkD,QAAA3B,GAAAP,KAAAsX,YAAA/W,GAAAxB,EAAAiD,OAIAhC,KAAAuX,SAAApW,OAAAQ,OAAA6W,GAAAnX,OAGAmI,GAAA,GACA,IAAAkP,EAAA1Y,KAAA2B,OACA+W,IAAAlW,OAAAgH,GACAxJ,KAAAmX,YAAAuB,EAAA/V,OAAAqU,MACAhX,KAAAuX,QAAApW,OAAAQ,OAAA3B,KAAAmX,aAAA9V,OAIA,MAEAsX,EAAAxX,OAAAQ,OAAA6W,GAAA5U,IAAAkT,GAAAnU,OAAA1D,MACA2Z,EAAAta,EAAA8Y,UAAAjW,OAAAQ,OAAAR,OAAAoK,UAAoEvL,KAAAqX,YAAAnU,EAAAmU,eACpEpV,OAJAlD,IAAA4Z,EAAAtC,KAAA9V,OAAAxB,EAAAiD,OAKAC,OAJAlD,IAAAiB,KAAAsX,YAAAvY,EAAAiD,OAKAW,OAAAqU,MAEAhX,KAAAqX,YAAAuB,EAGA,MAAAC,EAAA1X,OAAAQ,OAAA3B,KAAAqX,aAAA1U,OAAAoU,EAAA,GACA1O,EAAA,IAAA1B,EAAA3G,KAAAqD,GAAAiC,KAAAC,IAAAvF,KAAAqI,MAAAC,KAAAuQ,IAIA,OAFA7Y,KAAA2D,IAAAN,EACArD,KAAAyX,OAAApP,EACArI,KAOAJ,SACA,OACAyD,GAAArD,KAAAqD,GACA8E,MAAAnI,KAAAmI,MAAAvE,IAAAgT,IAIAhX,aACA,OACAyD,GAAArD,KAAAqD,GACA8E,MAAAnI,KAAAmI,MACAxG,OAAA3B,KAAA2B,QAOA/B,WACA,OAAAqO,EAAAC,KAAAnG,KAAAoG,UAAAnO,KAAA2H,WAWA/H,SAAAkZ,GACA,OAAA9Y,KAAA2B,OACAa,QACAuW,UACAnV,IAAA,CAAA7E,EAAAia,KACA,MACAC,EADAtU,EAAAuU,aAAAna,EAAAiB,KAAA2B,QACAN,OACA,IAAA8X,EAAA,IAAAtT,MAAAP,KAAAC,IAAA0T,EAAA,MAGA,OAFAE,EAAAF,EAAA,EAAAE,EAAAC,KAAA,MAAAD,GACAA,EAAAF,EAAA,EAAAE,EAAA/Z,QAAA,OAAA+Z,GACA9L,KAAA,KAAAyL,IAAA/Z,EAAAsW,SAAAtW,EAAAsW,WAEAhI,KAAA,MAQAzN,aAAAsD,GACA,YAAA3D,IAAA2D,EAAAG,SACA9D,IAAA2D,EAAAiF,YACA5I,IAAA2D,EAAAiU,YAOAvX,cACA,OAAA6W,EAAAjB,YAAAxV,KAAAiX,SAAAjX,MAWAJ,qBAAAiF,EAAA7C,EAAAX,GAAA,EAAA0D,EAAAzB,EAAA2B,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA7E,GAAA,UAAAsF,uBAA2DtF,KAG3D,OAAAyU,EAAApQ,cAAAxB,EAAA7C,EAAAX,EAAA0D,EAAAE,GACAhC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAAoD,EAAA5E,GAAA4E,EAAAtG,OAAAsG,EAAAE,MAAAF,EAAAI,MAAA/E,IAWA1D,qBAAAiF,EAAA7C,EAAAqB,EAAAhC,GAAA,EAAA0D,EAAAzB,EAAAlC,EAAA6D,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA7E,GAAA,UAAAsF,MAAA,0BAGA,OAAAmP,EAAAtT,cAAA0B,EAAA7C,EAAAqB,EAAAhC,EAAA0D,EAAAE,GACAhC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAAxB,EAAA4E,EAAAtG,OAAA,UAAA2B,EAAAlC,IAWAxB,gBAAAiF,EAAAkE,EAAA1H,GAAA,EAAAiC,EAAAlC,EAAA4D,EAAAC,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAQ,6BAGA,OAAAqP,EAAApG,SAAAxL,EAAAkE,EAAA1H,EAAAiC,EAAA0B,EAAAC,GACAhC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAAoD,EAAA5E,GAAA4E,EAAAtG,OAAA,UAAA2B,EAAAlC,IAYAxB,iBAAAiF,EAAAmE,EAAA3H,GAAA,EAAA0D,EAAAE,GACA,IAAA4B,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAAmC,GAAA,UAAA1B,MAAA,mCAGA,OAAAmP,EAAA4C,UAAAxU,EAAAmE,EAAA3H,EAAA0D,EAAAE,GACAhC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAAoD,EAAA5E,GAAA4E,EAAAtG,SAYA/B,kBAAAiF,EAAA3B,EAAAtB,EAAA6D,GAAA,GACA,IAAAoB,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA3D,GAAA,MAAA0D,EAAAS,qBACA,IAAAR,EAAAjF,GAAA,UAAA0F,MAAA,uCACA,IAAAhJ,EAAA8Z,MAAAlV,GAAA,MAAA0D,EAAAwB,eAEA,OAAAqO,EAAA6C,WAAAzU,EAAA3B,EAAAtB,EAAA6D,GACAxC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAA3B,EAAAG,GAAA4E,EAAAtG,OAAA,KAAAuB,EAAAmF,QAUAzI,cAAAiF,EAAA3B,EAAAuC,GACA,IAAAoB,EAAAhC,GAAA,MAAA+B,EAAAQ,6BACA,IAAAP,EAAA3D,GAAA,MAAA0D,EAAAS,qBACA,IAAA/I,EAAA8Z,MAAAlV,GAAA,MAAA0D,EAAAwB,eAEA,OAAAqO,EAAA8C,OAAA1U,EAAA3B,EAAAuC,GACAxC,KAAAgF,GAAA,IAAA3J,EAAAuG,EAAA3B,EAAAG,GAAA4E,EAAAtG,OAAAuB,EAAAiF,MAAAjF,EAAAmF,QAaAzI,iBAAAgC,GACA,IAMAW,EAAAX,EAAAe,OANA,CAAAzD,EAAAI,EAAA0Z,EAAAxT,KAGA,OADAlG,EAAAN,KAAAkD,QADAnD,GAAAG,EAAAH,GAAAO,EAAA0C,MAEA9C,OAQA,OAAA0C,EAAAK,OAHAlD,QAAAQ,IAAAgD,EAAAxD,EAAAiD,OAGAwG,KAFA,CAAAjI,EAAAC,IAAAD,EAAA8C,GAAA7C,EAAA6C,IAOAzD,iBAAAgC,GAEA,IAAA4X,KAEAC,KAEA3U,KAEApC,KAqBAd,EAAAM,QAnBAnD,IACA,IAAAA,EAAAC,KAAAqC,QACAoY,EAAAhW,KAAA1E,GASAA,EAAAC,KAAAkD,QAPA3B,IAEAiZ,EAAAjZ,KAAAiZ,EAAAjZ,OACAiZ,EAAAjZ,GAAAkD,KAAA1E,KAMA2D,IAAAtD,OAAAL,EAAAC,MAEA8F,EAAA/F,EAAAiD,OAAA,IAWA,MAAA0V,EAAAhV,EACAT,OALAlD,QAAAQ,IAAAuF,EAAA/F,IAMA6E,IALA7E,GAAAya,EAAAza,IAMA4D,OARA,CAAAzD,EAAA0C,EAAAoX,EAAAxT,IAAAtG,EAAAE,OAAA0H,EAAAlF,EAAA,aASAxC,OAAAqa,GAEA,OAAA3S,EAAA4Q,EAAA,QAAAlP,KAAA7D,EAAA8D,SAKA7I,sBAAAgC,GACA,IAAAkD,KAeA,OADAlD,EAAAM,QAbAnD,GAAA+F,EAAA/F,EAAAiD,OAAA,GAcAJ,EAAAe,OAZA,CAAAzD,EAAAI,EAAA0Z,EAAAxT,KAQA,OADAlG,EAAAN,KAAA+Z,UAAA7W,QANAnD,SAEAQ,IAAAuF,EAAA/F,IACAG,EAAAwa,OAAA,IAAA3a,KAIAG,QAQAQ,EAAAC,QAAArB","file":"1.bundle.js","sourcesContent":["const EventEmitter = require('events').EventEmitter\nconst pMap = require('p-map')\nconst Log = require('ipfs-log')\n\nconst Logger = require('logplease')\nconst logger = Logger.create(\"orbit-db.replicator\", { color: Logger.Colors.Cyan })\nLogger.setLogLevel('ERROR')\n\nconst sortClocks = (a, b) => (a.clock ? a.clock.time : a) - (b.clock ? b.clock.time : b)\nconst getNext = e => e.next\nconst flatMap = (res, val) => res.concat(val)\nconst notNull = entry => entry !== null && entry !== undefined\nconst uniqueValues = (res, val) => {\n  res[val] = val\n  return res\n}\n\nconst batchSize = 1\n\nclass Loader extends EventEmitter {\n  constructor (store, concurrency) {\n    super()\n    this._store = store\n    this._fetching = {}\n    this._stats = {\n      tasksRequested: 0,\n      tasksStarted: 0,\n      tasksProcessed: 0,\n      a: 0,\n      b: 0,\n      c: 0,\n      d: 0,\n    }\n    this._buffer = []\n\n    this._concurrency = concurrency || 128\n    this._queue = {}\n    this._q = new Set()\n\n    // Flush the queue as an emergency switch\n    this._flushTimer = setInterval(() => {\n      if (this.tasksRunning === 0 && Object.keys(this._queue).length > 0) {\n        logger.error(\"Had to flush the queue!\", Object.keys(this._queue).length, \"items in the queue, \", this.tasksRequested, this.tasksFinished, \" tasks requested/finished\")\n        setTimeout(() => this._processQueue(), 0)\n      }\n    }, 3000)\n  }\n\n  /**\n   * Returns the number of tasks started during the life time\n   * @return {[Integer]} [Number of tasks started]\n   */\n  get tasksRequested () {\n    return this._stats.tasksRequested\n  }\n\n  /**\n   * Returns the number of tasks started during the life time\n   * @return {[Integer]} [Number of tasks running]\n   */\n  get tasksStarted () {\n    return this._stats.tasksStarted\n  }\n\n  /**\n   * Returns the number of tasks running currently\n   * @return {[Integer]} [Number of tasks running]\n   */\n  get tasksRunning () {\n    return this._stats.tasksStarted - this._stats.tasksProcessed\n  }\n\n  /**\n   * Returns the number of tasks currently queued\n   * @return {[Integer]} [Number of tasks queued]\n   */\n  get tasksQueued () {\n    return Object.keys(this._queue).length - this.tasksRunning\n  }\n\n  /**\n   * Returns the number of tasks finished during the life time\n   * @return {[Integer]} [Number of tasks finished]\n   */\n  get tasksFinished () {\n    return this._stats.tasksProcessed\n  }\n\n  /**\n   * Returns the hashes currently queued\n   * @return {[Array<String>]} [Queued hashes]\n   */\n  getQueue () {\n    return Object.values(this._queue)\n  }\n\n  /*\n    Process new heads.\n   */\n  load (entries) {\n    this._stats.a += 1\n    const notKnown = entry => !this._store._oplog.has(entry.hash || entry) && !this._queue[entry.hash || entry]\n\n    try {\n      entries\n        .filter(notNull)\n        .filter(notKnown)\n        .forEach(this._addToQueue.bind(this))\n\n      setTimeout(() => this._processQueue(), 0)\n    } catch (e) {\n      console.error(e)\n    }\n    this._stats.a--\n  }\n\n  _addToQueue (entry) {\n    this._stats.b++\n    const hash = entry.hash || entry\n\n    if (this._store._oplog.has(hash) || this._fetching[hash] || this._queue[hash]) {\n      this._stats.b--\n      return\n    }\n\n    this._stats.tasksRequested += 1\n    this._queue[hash] = entry\n    this._stats.b--\n  }\n\n\n  async _processQueue () {\n    this._stats.c++\n    if (this.tasksRunning < this._concurrency) {\n      const capacity = this._concurrency - this.tasksRunning\n      // const items = Object.values(this._queue).sort(sortClocks).splice(0, capacity)\n      const items = Object.values(this._queue).slice(0, capacity).filter(notNull)\n      items.forEach(entry => delete this._queue[entry.hash || entry])\n\n      const flattenAndGetUniques = (nexts) => nexts.reduce(flatMap, []).reduce(uniqueValues, {})\n      const processValues = (nexts) => {\n        const values = Object.values(nexts).filter(notNull)\n        // logger.debug(\"Queue processed\", items.length, values.length, this._buffer.length, this.tasksRunning, this._stats.tasksRequested, this._stats.tasksProcessed)\n\n        if ((items.length > 0 && this._buffer.length > 0)\n          || (this.tasksRunning === 0 && this._buffer.length > 0)) {\n            const logs = this._buffer.slice()\n            this._buffer = []\n            // logger.debug(\"<load.end>\", \"[in/queued/running/out]\", this.tasksRequested, '/',  this.tasksQueued,  '/', this.tasksRunning, '/', this.tasksFinished)\n            this.emit('load.end', logs)\n        }\n\n        if (values.length > 0)\n          this.load(values)\n\n        this._stats.c--\n      }\n\n      return pMap(items, e => this._processOne(e))\n        .then(flattenAndGetUniques)\n        .then(processValues)\n    }\n  }\n\n  async _processOne (entry) {\n    this._stats.d++\n    const hash = entry.hash || entry\n\n    if (this._store._oplog.has(hash) || this._fetching[hash]) {\n      this._stats.d--\n      return\n    }\n\n    this._fetching[hash] = hash\n    this.emit('load.added', entry)\n    this._stats.tasksStarted += 1\n\n    // this._have = Object.assign({}, this._have, this._store._replicationInfo.have)\n\n    const exclude = []\n    const log = await Log.fromEntryHash(this._store._ipfs, hash, this._store._oplog.id, batchSize, exclude, this._store.key, this._store.access.write)\n    this._buffer.push(log)\n\n    const latest = log.values[0]\n    // this._have[latest.clock.time] = true\n    delete this._queue[hash]\n    // delete this._fetching[hash]\n\n\n    // Mark this task as processed\n    this._stats.tasksProcessed += 1\n\n    // logger.debug(\"<load.progress>\", \"[in/queued/running/out]\", this.tasksRequested, '/',  this.tasksQueued,  '/', this.tasksRunning, '/', this.tasksFinished)\n\n    // Notify subscribers that we made progress\n    this.emit('load.progress', this._id, hash, latest, null, this._buffer.length)\n\n    this._stats.d--\n    // Return all next pointers\n    return log.values.map(getNext).reduce(flatMap, [])\n  }\n}\n\nmodule.exports = Loader\n","'use strict'\n\n/*\n  Index\n\n  Index contains the state of a datastore, ie. what data we currently have.\n\n  Index receives a call from a Store when the operations log for the Store\n  was updated, ie. new operations were added. In updateIndex, the Index\n  implements its CRDT logic: add, remove or update items in the data\n  structure. Each new operation received from the operations log is applied\n  in order onto the current state, ie. each new operation changes the data\n  and the state changes.\n\n  Implementing each CRDT as an Index, we can implement both operation-based\n  and state-based CRDTs with the same higher level abstractions.\n\n  To read the current state of the database, Index provides a single public\n  function: `get()`. It is up to the Store to decide what kind of query\n  capabilities it provides to the consumer.\n\n  Usage:\n  ```javascript\n  const Index = new Index(userId)\n  ```\n*/\n\nclass Index {\n  /*\n    @param id - unique identifier of this index, eg. a user id or a hash\n  */\n  constructor(id) {\n    this.id = id\n    this._index = []\n  }\n\n  /*\n    Returns the state of the datastore, ie. most up-to-date data\n    @return - current state\n  */\n  get() {\n    return this._index\n  }\n\n  /*\n    Applies operations to the Index and updates the state\n    @param oplog - the source operations log that called updateIndex\n    @param entries - operations that were added to the log\n  */\n  updateIndex(oplog, entries) {\n    this._index = oplog.values\n  }\n}\n\nmodule.exports = Index\n","'use strict'\n\nfunction difference (a, b, key) {\n  // Indices for quick lookups\n  var processed = {}\n  var existing = {}\n\n  // Create an index of the first collection\n  var addToIndex = e => existing[key ? e[key] : e] = true\n  a.forEach(addToIndex)\n\n  // Reduce to entries that are not in the first collection\n  var reducer = (res, entry) => {\n    var isInFirst = existing[key ? entry[key] : entry] !== undefined\n    var hasBeenProcessed = processed[key ? entry[key] : entry] !== undefined\n    if (!isInFirst && !hasBeenProcessed) {\n      res.push(entry)\n      processed[key ? entry[key] : entry] = true\n    }\n    return res\n  }\n\n  return b.reduce(reducer, [])\n}\n\nmodule.exports = difference\n","'use strict'\n\nfunction intersection (a, b, key) {\n  // Indices for quick lookups\n  var processed = {}\n  var existing = {}\n\n  // Create an index of the first collection\n  var addToIndex = e => existing[key ? e[key] : e] = true\n  a.forEach(addToIndex)\n\n  // Reduce to entries that are not in the first collection\n  var reducer = (res, entry) => {\n    var isInFirst = existing[key ? entry[key] : entry] !== undefined\n    var hasBeenProcessed = processed[key ? entry[key] : entry] !== undefined\n    if (isInFirst && !hasBeenProcessed) {\n      res.push(entry)\n      processed[key ? entry[key] : entry] = true\n    }\n    return res\n  }\n\n  return b.reduce(reducer, [])\n}\n\nmodule.exports = intersection\n","'use strict';\n\nconst wrap = fn => new Promise(resolve => {\n\tresolve(fn());\n});\n\nmodule.exports = (condition, action) => wrap(function loop() {\n\tif (condition()) {\n\t\treturn wrap(action).then(loop);\n\t}\n});\n","'use strict'\n\nconst pWhilst = require('p-whilst')\nconst pMap = require('p-map')\nconst Entry = require('./entry')\n\nlet _tasksRequested = 0\nlet _tasksProcessed = 0\n\nclass EntryIO {\n  // Fetch log graphs in parallel\n  static fetchParallel (ipfs, hashes, length, exclude = [], concurrency, timeout, onProgressCallback) {\n    const fetchOne = (hash) => EntryIO.fetchAll(ipfs, hash, length, exclude, timeout, onProgressCallback)\n    const concatArrays = (arr1, arr2) => arr1.concat(arr2)\n    const flatten = (arr) => arr.reduce(concatArrays, [])\n    return pMap(hashes, fetchOne, { concurrency: Math.max(concurrency || hashes.length, 1) })\n      .then(flatten) // Flatten the results\n  }\n\n  /**\n   * Fetch log entries sequentially\n   *\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {string} [hash] Multihash of the entry to fetch\n   * @param {string} [parent] Parent of the node to be fetched\n   * @param {Object} [all] Entries to skip\n   * @param {Number} [amount=-1] How many entries to fetch\n   * @param {Number} [depth=0] Current depth of the recursion\n   * @param {function(hash, entry, parent, depth)} onProgressCallback\n   * @returns {Promise<Array<Entry>>}\n   */\n  static fetchAll (ipfs, hashes, amount, exclude = [], timeout = null, onProgressCallback) {\n    let result = []\n    let cache = {}\n    let loadingQueue = Array.isArray(hashes)\n      ? hashes.slice()\n      : [hashes]\n\n    // Add a multihash to the loading queue\n    const addToLoadingQueue = e => loadingQueue.push(e)\n\n    // Add entries that we don't need to fetch to the \"cache\"\n    var addToExcludeCache = e => cache[e.hash] = e\n    exclude.forEach(addToExcludeCache)\n\n    const shouldFetchMore = () => {\n      return loadingQueue.length > 0\n          && (result.length < amount || amount < 0)\n    }\n\n    const fetchEntry = () => {\n      const hash = loadingQueue.shift()\n\n      if (cache[hash]) {\n        return Promise.resolve()\n      }\n\n      return new Promise((resolve, reject) => {\n        // Resolve the promise after a timeout (if given) in order to\n        // not get stuck loading a block that is unreachable\n        const timer = timeout \n        ? setTimeout(() => {\n            console.warn(`Warning: Couldn't fetch entry '${hash}', request timed out (${timeout}ms)`)\n            resolve()\n          } , timeout) \n        : null\n\n        const addToResults = (entry) => {\n          clearTimeout(timer)\n          if (Entry.isEntry(entry)) {\n            entry.next.forEach(addToLoadingQueue)\n            result.push(entry)\n            cache[hash] = entry\n            _tasksProcessed ++\n            if (onProgressCallback) {\n              onProgressCallback(hash, entry, result.length)\n            }\n          }\n        }\n\n        _tasksRequested ++\n\n        // Load the entry\n        Entry.fromMultihash(ipfs, hash)\n          .then(addToResults)\n          .then(resolve)\n          .catch(err => {\n            resolve()\n          })\n      })\n    }\n\n    return pWhilst(shouldFetchMore, fetchEntry)\n      .then(() => result)\n  }\n}\n\nmodule.exports = EntryIO\n","'use strict'\n\nconst pMap = require('p-map')\nconst Entry = require('./entry')\nconst EntryIO = require('./entry-io')\nconst Clock = require('./lamport-clock')\nconst LogError = require('./log-errors')\nconst isDefined = require('./utils/is-defined')\nconst _uniques = require('./utils/uniques')\nconst intersection = require('./utils/intersection')\nconst difference = require('./utils/difference')\n\nconst last = (arr, n) => arr.slice(arr.length - n, arr.length)\n\nclass LogIO {\n  static toMultihash (immutabledb, log) {\n    if (!isDefined(immutabledb)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n\n    if (log.values.length < 1) throw new Error(`Can't serialize an empty log`)\n    // return this._storage.put(this.toBuffer())\n    return immutabledb.object.put(log.toBuffer())\n      .then((dagNode) => dagNode.toJSON().multihash)\n  }\n\n  /**\n   * Create a log from multihash\n   * @param {IPFS} ipfs - An IPFS instance\n   * @param {string} hash - Multihash (as a Base58 encoded string) to create the log from\n   * @param {Number} [length=-1] - How many items to include in the log\n   * @param {function(hash, entry, parent, depth)} onProgressCallback\n   * @returns {Promise<Log>}\n   */\n  static fromMultihash (immutabledb, hash, length = -1, exclude, onProgressCallback) {\n    if (!isDefined(immutabledb)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`)\n\n    return immutabledb.object.get(hash, { enc: 'base58' })\n      .then((dagNode) => JSON.parse(dagNode.toJSON().data))\n    // return immutabledb.get(hash)\n      .then((logData) => {\n        if (!logData.heads || !logData.id) throw LogError.NotALogError()\n        return EntryIO.fetchAll(immutabledb, logData.heads, length, exclude, null, onProgressCallback)\n          .then((entries) => {\n            // Find latest clock\n            const clock = entries.reduce((clock, entry) => {\n              if (entry.clock.time > clock.time) {\n                return new Clock(entry.clock.id, entry.clock.time)\n              }\n              return clock\n            }, new Clock(logData.id))\n            const finalEntries = entries.slice().sort(Entry.compare)\n            const heads = finalEntries.filter(e => logData.heads.includes(e.hash))\n            return {\n              id: logData.id,\n              values: finalEntries,\n              heads: heads,\n              clock: clock,\n            }\n          })\n      })\n  }\n\n  static fromEntryHash (ipfs, entryHash, id, length = -1, exclude, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.IpfsNotDefinedError()\n    if (!isDefined(entryHash)) throw new Error(\"'entryHash' must be defined\")\n\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, 1) : length\n\n    // Make sure we pass hashes instead of objects to the fetcher function\n    const excludeHashes = exclude// ? exclude.map(e => e.hash ? e.hash : e) : exclude\n\n    return EntryIO.fetchParallel(ipfs, [entryHash], length, excludeHashes, null, null, onProgressCallback)\n      .then((entries) => {\n        // Cap the result at the right size by taking the last n entries,\n        // or if given length is -1, then take all\n        const sliced = length > -1 ? last(entries, length) : entries\n        return {\n          values: sliced,\n        }\n      })\n  }\n\n  static fromJSON (ipfs, json, length = -1, key, timeout, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n\n    const mapper = (e, idx) => {\n      return Entry.create(ipfs, keystore, e.id, e.payload, e.next, e.clock, e.key)\n        .then((entry) => {\n          if (onProgressCallback) onProgressCallback(entry.hash, entry, idx + 1, json.values.length)\n          return entry\n        })\n    }\n\n    return EntryIO.fetchParallel(ipfs, json.heads.map(e => e.hash), length, [], 16, timeout, onProgressCallback)\n      .then((entries) => {\n        const finalEntries = entries.slice().sort(Entry.compare)\n        const heads = entries.filter(e => json.heads.includes(e.hash))\n        return {\n          id: json.id,\n          values: finalEntries,\n          heads: json.heads,\n        }\n      })\n  }\n\n  /**\n   * Create a new log starting from an entry\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Array<Entry>} entries An entry or an array of entries to fetch a log from\n   * @param {Number} [length=-1] How many entries to include. Default: infinite.\n   * @param {Array<Entry|string>} [exclude] Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} [onProgressCallback]\n   * @returns {Promise<Log>}\n   */\n  static fromEntry (immutabledb, sourceEntries, length = -1, exclude, key, keys, onProgressCallback) {\n    if (!isDefined(immutabledb)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\")\n\n    // Make sure we only have Entry objects as input\n    if (!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries)) {\n      throw new Error(`'sourceEntries' argument must be an array of Entry instances or a single Entry`)\n    }\n\n    if (!Array.isArray(sourceEntries)) {\n      sourceEntries = [sourceEntries]\n    }\n\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, sourceEntries.length) : length\n\n    // Make sure we pass hashes instead of objects to the fetcher function\n    const excludeHashes = exclude ? exclude.map(e => e.hash ? e.hash : e) : exclude\n    const hashes = sourceEntries.map(e => e.hash)\n\n    return EntryIO.fetchParallel(immutabledb, hashes, length, excludeHashes, null, null, onProgressCallback)\n      .then((entries) => {\n        var combined = sourceEntries.concat(entries)\n        var uniques = _uniques(combined, 'hash').sort(Entry.compare)\n\n        // Cap the result at the right size by taking the last n entries\n        const sliced = uniques.slice(length > -1 ? -length : -uniques.length)\n\n        // Make sure that the given input entries are present in the result\n        // in order to not lose references\n        const missingSourceEntries = difference(sliced, sourceEntries, 'hash')\n\n        const replaceInFront = (a, withEntries) => {\n          var sliced = a.slice(withEntries.length, a.length)\n          return withEntries.concat(sliced)\n        }\n\n        // Add the input entries at the beginning of the array and remove\n        // as many elements from the array before inserting the original entries\n        const result = replaceInFront(sliced, missingSourceEntries)\n        return {\n          id: result[result.length - 1].id,\n          values: result,\n        }\n      })\n  }\n\n  /**\n   * Expands the log with a specified number of new values\n   *\n   * @param  {[IPFS]} ipfs    [description]\n   * @param  {[Log]} log     [description]\n   * @param  {[Array<Entry>]} entries [description]\n   * @param  {Number} amount  [description]\n   * @return {[Log]}         [description]\n   */\n  static expandFrom (ipfs, log, entries, amount = -1) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!isDefined(entries)) throw new Error(`'entries' must be given as argument`)\n\n    if (!Array.isArray(entries)) {\n      entries = [entries]\n    }\n\n    // Hashes of the next references, array per entry,\n    // resulting an an array of arrays\n    const hashes = entries.map(e => e.next).filter(e => e.length > 0)\n\n    // If we don't have tails, we can't expand anymore, return\n    if (hashes.length === 0) {\n      return Promise.resolve({ values: log.values })\n    }\n\n    return EntryIO.fetchParallel(ipfs, hashes, amount, log.values)\n      .then((entries) => {\n        // Cap the length of the new collection (current length + wanted size)\n        // const size = amount > -1 ? (log.length + amount) : -1\n        const newEntries = log.merge(entries.slice(0, amount))\n\n        return {\n          values: newEntries,\n        }\n      })\n  }\n\n  static expand (ipfs, log, amount = -1) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n\n    // If we don't have any tails, we can't expand anymore\n    if (log.tailHashes.length === 0) {\n      return Promise.resolve({ values: log.values })\n    }\n\n    return EntryIO.fetchParallel(ipfs, log.tailHashes, amount, log.values)\n      .then((entries) => {\n        // Cap the length of the new collection (current length + wanted size)\n        const size = amount > -1 ? (log.values.length + amount) : -1\n\n        // Join the fetched entries with the log to order them first\n        const combined = log.values.concat(entries).sort(Entry.compare)\n        const sliced = (size > -1 ? combined.slice(-size) : combined.slice())\n\n        // Because the clocks can vary drastically, we need to make sure that\n        // we keep all the original entries in order to not lose references.\n        // So we do the following:\n        // 1) the old entries that are not in the sliced entries\n        // 2) New entries without the old entries\n        // 3) Entries that are in both\n\n        // These together are the entries we need to put back in\n        const missingOldEntries = difference(sliced, log.values, 'hash').sort(Entry.compare)\n        const withoutOldEntries = difference(log.values, sliced, 'hash').sort(Entry.compare)\n        const entryIntersection = intersection(log.values, sliced, 'hash').sort(Entry.compare)\n\n        // Calculate how many entries we keep from the remaining new entries\n        const length = size - (entryIntersection.length + missingOldEntries.length)\n        const remainingNewEntries = length > -1 ? withoutOldEntries.slice(-length) : withoutOldEntries\n\n        // Merge all the entries we want to keep\n        const merge = (a, b) => {\n          var combined = []\n          combined = a.concat(b)\n          var uniques = _uniques(combined, 'hash')\n          return uniques.sort(Entry.compare)\n        }\n\n        const merged = merge(missingOldEntries, merge(entryIntersection, remainingNewEntries))\n        return {\n          values: merged,\n        }\n      })\n  }\n\n}\n\nmodule.exports = LogIO\n","'use strict'\n\n/**\n * Interface for G-Set CRDT\n *\n * From:\n * \"A comprehensive study of Convergent and Commutative Replicated Data Types\"\n * https://hal.inria.fr/inria-00555588\n */\nclass GSet {\n  constuctor (values) {}\n  append (value) {}\n  merge (set) {}\n  get (value) {}\n  has (value) {}\n  get values () {}\n  get length () {}\n}\n\nmodule.exports = GSet\n","'use strict';\nmodule.exports = (iterable, reducer, initVal) => new Promise((resolve, reject) => {\n\tconst iterator = iterable[Symbol.iterator]();\n\tlet i = 0;\n\n\tconst next = total => {\n\t\tconst el = iterator.next();\n\n\t\tif (el.done) {\n\t\t\tresolve(total);\n\t\t\treturn;\n\t\t}\n\n\t\tPromise.all([total, el.value])\n\t\t\t.then(value => {\n\t\t\t\tnext(reducer(value[0], value[1], i++));\n\t\t\t})\n\t\t\t.catch(reject);\n\t};\n\n\tnext(initVal);\n});\n","'use strict';\nconst pReduce = require('p-reduce');\n\nmodule.exports = (iterable, iterator) => pReduce(iterable, (a, b, i) => iterator(b, i)).then(() => iterable);\n","'use strict'\n\nconst path = require('path')\nconst EventEmitter = require('events').EventEmitter\nconst Readable = require('readable-stream')\nconst mapSeries = require('p-each-series')\nconst Log = require('ipfs-log')\nconst Index = require('./Index')\nconst Loader = require('./Loader')\n\nconst Logger = require('logplease')\nconst logger = Logger.create(\"orbit-db.store\", { color: Logger.Colors.Blue })\nLogger.setLogLevel('ERROR')\n\nconst DefaultOptions = {\n  Index: Index,\n  maxHistory: -1,\n  path: './orbitdb',\n  replicate: true,\n  referenceCount: 64,\n  replicationConcurrency: 128,\n}\n\nclass Store {\n  constructor(ipfs, id, address, options) {\n    // Set the options\n    let opts = Object.assign({}, DefaultOptions)\n    Object.assign(opts, options)\n    this.options = opts\n\n    // Default type\n    this._type = 'store'\n\n    // Create IDs, names and paths\n    this.id = id\n    this.address = address\n    this.dbname = address.path || ''\n    this.events = new EventEmitter()\n\n    // External dependencies\n    this._ipfs = ipfs\n    this._cache = options.cache\n    this._index = new this.options.Index(this.id)\n\n    this._keystore = options.keystore \n    this._key = options && options.key\n      ? options.key\n      : this._keystore.getKey(id) || this._keystore.createKey(id)\n    // FIX: duck typed interface\n    this._ipfs.keystore = this._keystore \n\n    // Access mapping\n    const defaultAccess = { \n      admin: [this._key.getPublic('hex')], \n      read: [], // Not used atm, anyone can read\n      write: [this._key.getPublic('hex')] \n    }\n    this.access = options.accessController || defaultAccess\n\n    // Create the operations log\n    this._oplog = new Log(this._ipfs, this.id, null, null, null, this._key, this.access.write)\n\n    // Replication progress info\n    this._replicationInfo = {\n      buffered: 0,\n      queued: 0,\n      progress: 0,\n      max: 0,\n    }\n\n    // Statistics\n    this._stats = {\n      snapshot: {\n        bytesLoaded: -1,\n      },\n      syncRequestsReceieved: 0,\n    }\n\n    try {\n      this._loader = new Loader(this, this.options.replicationConcurrency)\n      this._loader.on('load.added', (entry) => {\n        // Update the latest entry state (latest is the entry with largest clock time)\n        this._replicationInfo.queued ++\n        this._replicationInfo.max = Math.max.apply(null, [this._replicationInfo.max, this._oplog.length, entry.clock ? entry.clock.time : 0])\n        // logger.debug(`<replicate>`)\n        this.events.emit('replicate', this.address.toString(), entry)\n      })\n      this._loader.on('load.progress', (id, hash, entry, have, bufferedLength) => {\n        // console.log(\">>\", this._oplog.length, this._replicationInfo.progress, this._replicationInfo.buffered, bufferedLength)\n        if (this._replicationInfo.buffered > bufferedLength) {\n          this._replicationInfo.progress = this._replicationInfo.progress + bufferedLength\n        } else {\n          this._replicationInfo.progress = Math.max.apply(null, [this._oplog.length, this._replicationInfo.progress, this._oplog.length + bufferedLength])\n        }\n        // console.log(\">>>\", this._replicationInfo.progress)\n        this._replicationInfo.buffered = bufferedLength\n        this._replicationInfo.max = Math.max.apply(null, [this._replicationInfo.max, this._replicationInfo.progress])\n        // logger.debug(`<replicate.progress>`)\n        this.events.emit('replicate.progress', this.address.toString(), hash, entry, this._replicationInfo.progress, have)\n      })\n\n      const onLoadCompleted = async (logs, have) => {\n        try {\n          for (let log of logs) {\n            await this._oplog.join(log, -1, this._oplog.id)\n          }\n          this._replicationInfo.max = Math.max(this._replicationInfo.max, this._oplog.length)\n          this._index.updateIndex(this._oplog)\n          this._replicationInfo.progress = Math.max.apply(null, [this._replicationInfo.progress, this._oplog.length])\n          this._replicationInfo.queued -= logs.length\n          // logger.debug(`<replicated>`)\n          this.events.emit('replicated', this.address.toString(), logs.length)\n        } catch (e) {\n          console.error(e)\n        }\n      }\n      this._loader.on('load.end', onLoadCompleted)\n    } catch (e) {\n      console.error(\"Store Error:\", e)\n    }\n  }\n\n  get all () {\n    return Array.isArray(this._index._index) \n      ? this._index._index \n      : Object.keys(this._index._index).map(e => this._index._index[e])\n  }\n\n  get type () {\n    return this._type\n  }\n\n  get key () {\n    return this._key\n  }\n\n  async close () {\n    // Reset replication statistics\n    this._replicationInfo = {\n      buffered: 0,\n      queued: 0,\n      progress: 0,\n      max: 0,\n    }\n    // Reset database statistics\n    this._stats = {\n      snapshot: {\n        bytesLoaded: -1,\n      },\n      syncRequestsReceieved: 0,\n    }\n    // Remove all event listeners\n    this.events.removeAllListeners('load')\n    this.events.removeAllListeners('load.progress')\n    this.events.removeAllListeners('replicate')\n    this.events.removeAllListeners('replicate.progress')\n    this.events.removeAllListeners('replicated')\n    this.events.removeAllListeners('ready')\n    this.events.removeAllListeners('write')\n\n    // Close cache\n    await this._cache.close()\n\n    // Database is now closed\n    this.events.emit('closed', this.address.toString())\n    return Promise.resolve()\n  }\n\n  async drop () {\n    await this.close()\n    await this._cache.destroy()\n    this._index = new this.options.Index(this.id)\n    this._oplog = new Log(this._ipfs, this.id, null, null, null, this._key, this.access.write)\n    this._cache = this.options.cache\n  }\n\n  async load (amount) {\n    amount = amount ? amount : this.options.maxHistory\n\n    const localHeads = await this._cache.get('_localHeads') || []\n    const remoteHeads = await this._cache.get('_remoteHeads') || []\n    const heads = localHeads.concat(remoteHeads)\n\n    if (heads.length > 0)\n      this.events.emit('load', this.address.toString(), heads)\n\n    await mapSeries(heads, async (head) => {\n      this._replicationInfo.max = Math.max(this._replicationInfo.max, head.clock.time)\n      let log = await Log.fromEntryHash(this._ipfs, head.hash, this._oplog.id, amount, this._oplog.values, this.key, this.access.write, this._onLoadProgress.bind(this))\n      await this._oplog.join(log, amount, this._oplog.id)\n      this._replicationInfo.progress = Math.max.apply(null, [this._replicationInfo.progress, this._oplog.length])\n      this._replicationInfo.max = Math.max.apply(null, [this._replicationInfo.max, this._replicationInfo.progress])\n    })\n\n    // Update the index\n    if (heads.length > 0)\n      this._index.updateIndex(this._oplog)\n\n    this.events.emit('ready', this.address.toString(), this._oplog.heads)\n  }\n\n  sync (heads) {\n    this._stats.syncRequestsReceieved += 1\n    logger.debug(`Sync request #${this._stats.syncRequestsReceieved} ${heads.length}`)\n\n    if (heads.length === 0)\n      return\n\n    // To simulate network latency, uncomment this line\n    // and comment out the rest of the function\n    // That way the object (received as head message from pubsub)\n    // doesn't get written to IPFS and so when the Loader is fetching\n    // the log, it'll fetch it from the network instead from the disk.\n    // return this._loader.load(heads)\n\n    const saveToIpfs = (head) => {\n      if (!head) {\n        console.warn(\"Warning: Given input entry was 'null'.\")\n        return Promise.resolve(null)\n      }\n\n      if (!this.access.write.includes(head.key) && !this.access.write.includes('*')) {\n        console.warn(\"Warning: Given input entry is not allowed in this log and was discarded (no write access).\")\n        return Promise.resolve(null)\n      }\n\n      // TODO: verify the entry's signature here\n\n      const logEntry = Object.assign({}, head)\n      logEntry.hash = null\n      return this._ipfs.object.put(Buffer.from(JSON.stringify(logEntry)))\n        .then((dagObj) => dagObj.toJSON().multihash)\n        .then(hash => {\n          // We need to make sure that the head message's hash actually\n          // matches the hash given by IPFS in order to verify that the\n          // message contents are authentic\n          if (hash !== head.hash) {\n            console.warn('\"WARNING! Head hash didn\\'t match the contents')\n          }\n\n          return hash\n        })\n        .then(() => head)\n    }\n\n    return mapSeries(heads, saveToIpfs)\n      .then(async (saved) => {\n        await this._cache.set('_remoteHeads', heads)\n        logger.debug(`Saved heads ${heads.length} [${saved.map(e => e.hash).join(', ')}]`)\n        return this._loader.load(saved.filter(e => e !== null))\n      })\n  }\n\n  loadMoreFrom (amount, entries) {\n    this._loader.load(entries)\n  }\n\n  async saveSnapshot() {\n    const unfinished = this._loader.getQueue()\n\n    let snapshotData = this._oplog.toSnapshot()\n    let header = new Buffer(JSON.stringify({\n      id: snapshotData.id,\n      heads: snapshotData.heads,\n      size: snapshotData.values.length,\n      type: this.type,\n    }))\n    const rs = new Readable()\n    let size = new Uint16Array([header.length])\n    let bytes = new Buffer(size.buffer)\n    rs.push(bytes)\n    rs.push(header)\n\n    const addToStream = (val) => {\n      let str = new Buffer(JSON.stringify(val))\n      let size = new Uint16Array([str.length])\n      rs.push(new Buffer(size.buffer))\n      rs.push(str)\n    }\n\n    snapshotData.values.forEach(addToStream)\n    rs.push(null) // tell the stream we're finished\n\n    const stream = {\n      path: this.address.toString(),\n      content: rs\n    }\n\n    const snapshot = await this._ipfs.files.add(stream)\n\n    await this._cache.set('snapshot', snapshot[snapshot.length - 1])\n    await this._cache.set('queue', unfinished)\n\n    logger.debug(`Saved snapshot: ${snapshot[snapshot.length - 1].hash}, queue length: ${unfinished.length}`)\n\n    return snapshot\n  }\n\n  async loadFromSnapshot (onProgressCallback) {\n    this.events.emit('load', this.address.toString())\n\n    const queue = await this._cache.get('queue')\n    this.sync(queue || [])\n\n    const snapshot = await this._cache.get('snapshot')\n\n    if (snapshot) {\n      const res = await this._ipfs.files.catReadableStream(snapshot.hash)\n      const loadSnapshotData = () => {\n        return new Promise((resolve, reject) => {\n          let buf = new Buffer(0)\n          let q = []\n\n          const bufferData = (d) => {\n            this._byteSize += d.length\n            if (q.length < 20000) {\n              q.push(d)\n            } else {\n              const a = Buffer.concat(q)\n              buf = Buffer.concat([buf, a])\n              q = []\n            }\n          }\n\n          const done = () => {\n            // this.events.emit('load.progress', this.address.toString(), null, null, 0, this._replicationInfo.max)\n\n            if (q.length > 0) {\n              const a = Buffer.concat(q)\n              buf = Buffer.concat([buf, a])\n            }\n\n            function toArrayBuffer(buf) {\n              var ab = new ArrayBuffer(buf.length)\n              var view = new Uint8Array(ab)\n              for (var i = 0; i < buf.length; ++i) {\n                  view[i] = buf[i]\n              }\n              return ab\n            }\n\n            const headerSize = parseInt(new Uint16Array(toArrayBuffer(buf.slice(0, 2))))\n            let header\n\n            try {\n              header = JSON.parse(buf.slice(2, headerSize + 2))\n            } catch (e) {\n              //TODO\n            }\n\n            let values = []\n            let a = 2 + headerSize\n            while(a < buf.length) {\n              const s = parseInt(new Uint16Array(toArrayBuffer(buf.slice(a, a + 2))))\n              // console.log(\"size: \", s)\n              a += 2\n              const data = buf.slice(a, a + s)\n              try {\n                const d = JSON.parse(data)\n                values.push(d)\n              } catch (e) {\n              }\n              a += s\n            }\n\n            if (header) {\n              this._type = header.type\n              this._replicationInfo.max = Math.max(this._replicationInfo.max, values.reduce((res, val) => Math.max(res, val.clock.time), 0))\n              resolve({ values: values, id: header.id, heads: header.heads, type: header.type })\n            } else {\n              this._replicationInfo.max = 0\n              resolve({ values: values, id: null, heads: null, type: null })\n            }\n          }\n          res.on('data', bufferData)\n          res.on('end', done)\n        })\n      }\n\n      const onProgress = (hash, entry, count, total) => {\n        this._replicationInfo.max = Math.max(this._replicationInfo.max, entry.clock.time)\n        this._replicationInfo.progress = Math.max.apply(null, [this._replicationInfo.progress, count, this._oplog.length])\n        this._onLoadProgress(hash, entry, this._replicationInfo.progress, this._replicationInfo.max)\n      }\n\n      // Fetch the entries\n      // Timeout 1 sec to only load entries that are already fetched (in order to not get stuck at loading)\n      const snapshotData = await loadSnapshotData()\n      if (snapshotData) {\n        const log = await Log.fromJSON(this._ipfs, snapshotData, -1, this._key, this.access.write, 1000, onProgress)\n        await this._oplog.join(log, -1, this._oplog.id)\n        this._replicationInfo.max = Math.max.apply(null, [this._replicationInfo.max, this._replicationInfo.progress, this._oplog.length])\n        this._replicationInfo.progress = Math.max(this._replicationInfo.progress, this._oplog.length)\n        this._index.updateIndex(this._oplog)\n        this.events.emit('replicated', this.address.toString())\n      }\n      this.events.emit('ready', this.address.toString(), this._oplog.heads)\n    } else {\n      throw new Error(`Snapshot for ${this.address} not found!`)\n    }\n    return this\n  }\n\n  async _addOperation(data, batchOperation, lastOperation, onProgressCallback) {\n    if(this._oplog) {\n      const entry = await this._oplog.append(data, this.options.referenceCount)\n      this._replicationInfo.progress++\n      this._replicationInfo.max = Math.max.apply(null, [this._replicationInfo.max, this._replicationInfo.progress, entry.clock.time])\n      const address = this.address.toString()\n      await this._cache.set('_localHeads', [entry])\n      this._index.updateIndex(this._oplog)\n      this.events.emit('write', this.address.toString(), entry, this._oplog.heads)\n      if (onProgressCallback) onProgressCallback(entry)\n      return entry.hash\n    }\n  }\n\n  _addOperationBatch(data, batchOperation, lastOperation, onProgressCallback) {\n    throw new Error(\"Not implemented!\")\n  }\n\n  _onLoadProgress (hash, entry, progress, total) {\n    this.events.emit('load.progress', this.address.toString(), hash, entry, Math.max(this._oplog.length, progress), Math.max(this._oplog.length || 0, this._replicationInfo.max || 0))\n  }\n}\n\nmodule.exports = Store\n","'use strict';\n\nconst fs = require('fs');\nconst format = require('util').format;\nconst EventEmitter = require('events').EventEmitter;\n\nlet isNodejs = process.version ? true : false;\n\nconst LogLevels = {\n  'DEBUG': 'DEBUG',\n  'INFO':  'INFO',\n  'WARN':  'WARN',\n  'ERROR': 'ERROR',\n  'NONE':  'NONE',\n};\n\n// Global log level\nlet GlobalLogLevel = LogLevels.DEBUG;\n\n// Global log file name\nlet GlobalLogfile = null;\n\nlet GlobalEvents = new EventEmitter();\n\n// ANSI colors\nlet Colors = {\n  'Black':   0,\n  'Red':     1,\n  'Green':   2,\n  'Yellow':  3,\n  'Blue':    4,\n  'Magenta': 5,\n  'Cyan':    6,\n  'Grey':    7,\n  'White':   9,\n  'Default': 9,\n};\n\n// CSS colors\nif(!isNodejs) {\n  Colors = {\n    'Black':   'Black',\n    'Red':     'IndianRed',\n    'Green':   'LimeGreen',\n    'Yellow':  'Orange',\n    'Blue':    'RoyalBlue',\n    'Magenta': 'Orchid',\n    'Cyan':    'SkyBlue',\n    'Grey':    'DimGrey',\n    'White':   'White',\n    'Default': 'Black',\n  };\n}\n\nconst loglevelColors = [Colors.Cyan, Colors.Green, Colors.Yellow, Colors.Red, Colors.Default];\n\nconst defaultOptions = {\n  useColors: true,\n  color: Colors.Default,\n  showTimestamp: true,\n  showLevel: true,\n  filename: GlobalLogfile,\n  appendFile: true,\n};\n\nclass Logger {\n  constructor(category, options) {\n    this.category = category;\n    let opts = {};\n    Object.assign(opts, defaultOptions);\n    Object.assign(opts, options);\n    this.options = opts;\n  }\n\n  debug() {\n    if(this._shouldLog(LogLevels.DEBUG))\n      this._write(LogLevels.DEBUG, format.apply(null, arguments));\n  }\n\n  log() {\n    if(this._shouldLog(LogLevels.DEBUG))\n      this.debug.apply(this, arguments);\n  }\n\n  info() {\n    if(this._shouldLog(LogLevels.INFO))\n      this._write(LogLevels.INFO, format.apply(null, arguments));\n  }\n\n  warn() {\n    if(this._shouldLog(LogLevels.WARN))\n      this._write(LogLevels.WARN, format.apply(null, arguments));\n  }\n\n  error() {\n    if(this._shouldLog(LogLevels.ERROR))\n      this._write(LogLevels.ERROR, format.apply(null, arguments));\n  }\n\n  _write(level, text) {\n    if((this.options.filename || GlobalLogfile) && !this.fileWriter && isNodejs)\n      this.fileWriter = fs.openSync(this.options.filename || GlobalLogfile, this.options.appendFile ? 'a+' : 'w+');\n\n    let format = this._format(level, text);\n    let unformattedText = this._createLogMessage(level, text);\n    let formattedText = this._createLogMessage(level, text, format.timestamp, format.level, format.category, format.text);\n\n    if(this.fileWriter && isNodejs)\n      fs.writeSync(this.fileWriter, unformattedText + '\\n', null, 'utf-8');\n\n    if(isNodejs || !this.options.useColors) {\n      console.log(formattedText)\n      GlobalEvents.emit('data', this.category, level, text)\n    } else {\n      // TODO: clean this up\n      if(level === LogLevels.ERROR) {\n        if(this.options.showTimestamp && this.options.showLevel) {\n          console.error(formattedText, format.timestamp, format.level, format.category, format.text)\n        } else if(this.options.showTimestamp && !this.options.showLevel) {\n          console.error(formattedText, format.timestamp, format.category, format.text)\n        } else if(!this.options.showTimestamp && this.options.showLevel) {\n          console.error(formattedText, format.level, format.category, format.text)\n        } else {\n          console.error(formattedText, format.category, format.text)\n        }\n      } else {\n        if(this.options.showTimestamp && this.options.showLevel) {\n          console.log(formattedText, format.timestamp, format.level, format.category, format.text)\n        } else if(this.options.showTimestamp && !this.options.showLevel) {\n          console.log(formattedText, format.timestamp, format.category, format.text)\n        } else if(!this.options.showTimestamp && this.options.showLevel) {\n          console.log(formattedText, format.level, format.category, format.text)\n        } else {\n          console.log(formattedText, format.category, format.text)\n        }\n      }\n    }\n  }\n\n  _format(level, text) {\n    let timestampFormat = '';\n    let levelFormat     = '';\n    let categoryFormat  = '';\n    let textFormat      = ': ';\n\n    if(this.options.useColors) {\n        const levelColor    = Object.keys(LogLevels).map((f) => LogLevels[f]).indexOf(level);\n        const categoryColor = this.options.color;\n\n      if(isNodejs) {\n        if(this.options.showTimestamp)\n          timestampFormat = '\\u001b[3' + Colors.Grey + 'm';\n\n        if(this.options.showLevel)\n          levelFormat = '\\u001b[3' + loglevelColors[levelColor] + ';22m';\n\n        categoryFormat = '\\u001b[3' + categoryColor + ';1m';\n        textFormat = '\\u001b[0m: ';\n      } else {\n        if(this.options.showTimestamp)\n          timestampFormat = 'color:' + Colors.Grey;\n\n        if(this.options.showLevel)\n          levelFormat = 'color:' + loglevelColors[levelColor];\n\n        categoryFormat = 'color:' + categoryColor + '; font-weight: bold';\n      }\n    }\n\n    return {\n      timestamp: timestampFormat,\n      level: levelFormat,\n      category: categoryFormat,\n      text: textFormat\n    };\n  }\n\n  _createLogMessage(level, text, timestampFormat, levelFormat, categoryFormat, textFormat) {\n    timestampFormat = timestampFormat || '';\n    levelFormat     = levelFormat     || '';\n    categoryFormat  = categoryFormat  || '';\n    textFormat      = textFormat      || ': ';\n\n    if(!isNodejs && this.options.useColors) {\n      if(this.options.showTimestamp)\n        timestampFormat = '%c';\n\n      if(this.options.showLevel)\n        levelFormat = '%c';\n\n      categoryFormat  = '%c';\n      textFormat = ': %c';\n    }\n\n    let result = '';\n\n    if(this.options.showTimestamp)\n      result += '' + new Date().toISOString() + ' ';\n\n    result = timestampFormat + result;\n\n    if(this.options.showLevel)\n      result += levelFormat + '[' + level +']' + (level === LogLevels.INFO || level === LogLevels.WARN ? ' ' : '') + ' ';\n\n    result += categoryFormat + this.category;\n    result += textFormat + text;\n    return result;\n  }\n\n  _shouldLog(level) {\n    let envLogLevel = (typeof process !== \"undefined\" && process.env !== undefined && process.env.LOG !== undefined) ? process.env.LOG.toUpperCase() : null;\n    envLogLevel = (typeof window !== \"undefined\" && window.LOG) ? window.LOG.toUpperCase() : envLogLevel;\n\n    const logLevel = envLogLevel || GlobalLogLevel;\n    const levels   = Object.keys(LogLevels).map((f) => LogLevels[f]);\n    const index    = levels.indexOf(level);\n    const levelIdx = levels.indexOf(logLevel);\n    return index >= levelIdx;\n  }\n};\n\n/* Public API */\nmodule.exports = {\n  Colors: Colors,\n  LogLevels: LogLevels,\n  setLogLevel: (level) => {\n    GlobalLogLevel = level;\n  },\n  setLogfile: (filename) => {\n    GlobalLogfile = filename;\n  },\n  create: (category, options) => {\n    const logger = new Logger(category, options);\n    return logger;\n  },\n  forceBrowserMode: (force) => isNodejs = !force, // for testing,\n  events: GlobalEvents,\n};\n","'use strict';\nmodule.exports = (iterable, mapper, opts) => new Promise((resolve, reject) => {\n\topts = Object.assign({\n\t\tconcurrency: Infinity\n\t}, opts);\n\n\tif (typeof mapper !== 'function') {\n\t\tthrow new TypeError('Mapper function is required');\n\t}\n\n\tconst concurrency = opts.concurrency;\n\n\tif (!(typeof concurrency === 'number' && concurrency >= 1)) {\n\t\tthrow new TypeError(`Expected \\`concurrency\\` to be a number from 1 and up, got \\`${concurrency}\\` (${typeof concurrency})`);\n\t}\n\n\tconst ret = [];\n\tconst iterator = iterable[Symbol.iterator]();\n\tlet isRejected = false;\n\tlet iterableDone = false;\n\tlet resolvingCount = 0;\n\tlet currentIdx = 0;\n\n\tconst next = () => {\n\t\tif (isRejected) {\n\t\t\treturn;\n\t\t}\n\n\t\tconst nextItem = iterator.next();\n\t\tconst i = currentIdx;\n\t\tcurrentIdx++;\n\n\t\tif (nextItem.done) {\n\t\t\titerableDone = true;\n\n\t\t\tif (resolvingCount === 0) {\n\t\t\t\tresolve(ret);\n\t\t\t}\n\n\t\t\treturn;\n\t\t}\n\n\t\tresolvingCount++;\n\n\t\tPromise.resolve(nextItem.value)\n\t\t\t.then(el => mapper(el, i))\n\t\t\t.then(\n\t\t\t\tval => {\n\t\t\t\t\tret[i] = val;\n\t\t\t\t\tresolvingCount--;\n\t\t\t\t\tnext();\n\t\t\t\t},\n\t\t\t\terr => {\n\t\t\t\t\tisRejected = true;\n\t\t\t\t\treject(err);\n\t\t\t\t}\n\t\t\t);\n\t};\n\n\tfor (let i = 0; i < concurrency; i++) {\n\t\tnext();\n\n\t\tif (iterableDone) {\n\t\t\tbreak;\n\t\t}\n\t}\n});\n","'use strict'\n\nconst isDefined = (arg) => arg !== undefined && arg !== null\n\nmodule.exports = isDefined\n","'use strict'\n\nclass LamportClock {\n  constructor (id, time) {\n    this.id = id\n    this.time = time || 0\n  }\n\n  tick () {\n    return new LamportClock(this.id, ++this.time)\n  }\n\n  merge (clock) {\n    this.time = Math.max(this.time, clock.time)\n    return new LamportClock(this.id, this.time)\n  }\n\n  clone () {\n    return new LamportClock(this.id, this.time)\n  }\n\n  static compare (a, b) {\n    // Calculate the \"distance\" based on the clock, ie. lower or greater\n    var dist = a.time - b.time\n\n    // If the sequence number is the same (concurrent events),\n    // and the IDs are different, take the one with a \"lower\" id\n    if (dist === 0 && a.id !== b.id) return a.id < b.id ? -1 : 1\n\n    return dist\n  }\n}\n\nmodule.exports = LamportClock\n","'use strict'\n\nconst Clock = require('./lamport-clock')\nconst isDefined = require('./utils/is-defined')\n\nconst IpfsNotDefinedError = () => new Error('Ipfs instance not defined')\n\nclass Entry {\n  /**\n   * Create an Entry\n   * @param {IPFS} ipfs - An IPFS instance\n   * @param {string|Buffer|Object|Array} data - Data of the entry to be added. Can be any JSON.stringifyable data.\n   * @param {Array<Entry|string>} [next=[]] Parents of the entry\n   * @example\n   * const entry = await Entry.create(ipfs, 'hello')\n   * console.log(entry)\n   * // { hash: \"Qm...Foo\", payload: \"hello\", next: [] }\n   * @returns {Promise<Entry>}\n   */\n  static async create (ipfs, id, data, next = [], clock, signKey) {\n    if (!isDefined(ipfs)) throw IpfsNotDefinedError()\n    if (!isDefined(id)) throw new Error('Entry requires an id')\n    if (!isDefined(data)) throw new Error('Entry requires data')\n    if (!isDefined(next) || !Array.isArray(next)) throw new Error(\"'next' argument is not an array\")\n\n    // Clean the next objects and convert to hashes\n    const toEntry = (e) => e.hash ? e.hash : e\n    let nexts = next.filter(isDefined)\n      .map(toEntry)\n\n    let entry = {\n      hash: null, // \"Qm...Foo\", we'll set the hash after persisting the entry\n      id: id, // For determining a unique chain\n      payload: data, // Can be any JSON.stringifyable data\n      next: nexts, // Array of Multihashes\n      v: 0, // For future data structure updates, should currently always be 0\n      clock: new Clock(id, clock ? clock.time : null),\n    }\n\n    // If signing key was passedd, sign the enrty\n    if (ipfs.keystore && signKey) {\n      entry = await Entry.signEntry(ipfs.keystore, entry, signKey) \n    }\n\n    entry.hash = await Entry.toMultihash(ipfs, entry)\n    return entry\n  }\n\n  static async signEntry (keystore, entry, key) {\n    const signature = await keystore.sign(key, new Buffer(JSON.stringify(entry)))\n    entry.sig = signature\n    entry.key = key.getPublic('hex')\n    return entry\n  }\n\n  static async verifyEntry (entry, keystore) {\n    const e = Object.assign({}, {\n      hash: null,\n      id: entry.id,\n      payload: entry.payload,\n      next: entry.next,\n      v: entry.v,\n      clock: entry.clock,\n    })\n\n    const pubKey = await keystore.importPublicKey(entry.key)\n    await keystore.verify(entry.sig, pubKey, new Buffer(JSON.stringify(e)))\n  }\n\n  /**\n   * Get the multihash of an Entry\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {Entry} [entry] Entry to get a multihash for\n   * @example\n   * const hash = await Entry.toMultihash(ipfs, entry)\n   * console.log(hash)\n   * // \"Qm...Foo\"\n   * @returns {Promise<string>}\n   */\n  static toMultihash (ipfs, entry) {\n    if (!ipfs) throw IpfsNotDefinedError()\n    const data = Buffer.from(JSON.stringify(entry))\n    return ipfs.object.put(data)\n      .then((res) => res.toJSON().multihash)\n  }\n\n  /**\n   * Create an Entry from a multihash\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {string} [hash] Multihash as Base58 encoded string to create an Entry from\n   * @example\n   * const hash = await Entry.fromMultihash(ipfs, \"Qm...Foo\")\n   * console.log(hash)\n   * // { hash: \"Qm...Foo\", payload: \"hello\", next: [] }\n   * @returns {Promise<Entry>}\n   */\n  static fromMultihash (ipfs, hash) {\n    if (!ipfs) throw IpfsNotDefinedError()\n    if (!hash) throw new Error(`Invalid hash: ${hash}`)\n    return ipfs.object.get(hash, { enc: 'base58' })\n      .then((obj) => JSON.parse(obj.toJSON().data))\n      .then((data) => {\n        let entry = {\n          hash: hash,\n          id: data.id,\n          payload: data.payload,\n          next: data.next,\n          v: data.v,\n          clock: data.clock,\n        }\n        if (data.sig) Object.assign(entry, { sig: data.sig })\n        if (data.key) Object.assign(entry, { key: data.key })\n        return entry\n      })\n  }\n\n  /**\n   * Check if an object is an Entry\n   * @param {Entry} obj\n   * @returns {boolean}\n   */\n  static isEntry (obj) {\n    return obj.id !== undefined\n      && obj.next !== undefined\n      && obj.hash !== undefined\n      && obj.payload !== undefined\n      && obj.v !== undefined\n      && obj.clock !== undefined\n  }\n\n  static compare (a, b) {\n    var distance = Clock.compare(a.clock, b.clock)\n    if (distance === 0) return a.clock.id < b.clock.id ? -1 : 1\n    return distance\n  }\n\n  /**\n   * Check if an entry equals another entry\n   * @param {Entry} a\n   * @param {Entry} b\n   * @returns {boolean}\n   */\n  static isEqual (a, b) {\n    return a.hash === b.hash\n  }\n\n  /**\n   * Check if an entry is a parent to another entry.\n   * @param {Entry} [entry1] Entry to check\n   * @param {Entry} [entry2] Parent\n   * @returns {boolean}\n   */\n  static isParent (entry1, entry2) {\n    return entry2.next.indexOf(entry1.hash) > -1\n  }\n\n  /**\n   * Find entry's children from an Array of entries\n   *\n   * @description\n   * Returns entry's children as an Array up to the last know child.\n   *\n   * @param {Entry} [entry] Entry for which to find the parents\n   * @param {Array<Entry>} [vaules] Entries to search parents from\n   * @returns {Array<Entry>}\n   */\n  static findChildren (entry, values) {\n    var stack = []\n    var parent = values.find((e) => Entry.isParent(entry, e))\n    var prev = entry\n    while (parent) {\n      stack.push(parent)\n      prev = parent\n      parent = values.find((e) => Entry.isParent(prev, e))\n    }\n    stack = stack.sort((a, b) => a.clock.time > a.clock.time)\n    return stack\n  }\n}\n\nmodule.exports = Entry\n","'use strict'\n\nfunction uniques (value, key) {\n  // Create an index of the collection\n  let uniques = {}\n  var get = e => uniques[e]\n  var addToIndex = e => uniques[key ? e[key] : e] = e\n  value.forEach(addToIndex)\n  return Object.keys(uniques).map(get)\n}\n\nmodule.exports = uniques\n","'use strict'\n\nconst ImmutableDBNotDefinedError = () => new Error('ImmutableDB instance not defined')\nconst LogNotDefinedError = () => new Error('Log instance not defined')\nconst NotALogError = () => new Error('Given argument is not an instance of Log')\n\nmodule.exports = {\n  ImmutableDBNotDefinedError: ImmutableDBNotDefinedError,\n  LogNotDefinedError: LogNotDefinedError,\n  NotALogError: NotALogError,\n}\n","'use strict'\n\nconst pMap = require('p-map')\nconst GSet = require('./g-set')\nconst Entry = require('./entry')\nconst LogIO = require('./log-io')\nconst LogError = require('./log-errors')\nconst Clock = require('./lamport-clock')\nconst isDefined = require('./utils/is-defined')\nconst _uniques = require('./utils/uniques')\n\nconst randomId = () => new Date().getTime().toString()\nconst getHash = e => e.hash\nconst flatMap = (res, acc) => res.concat(acc)\nconst getNextPointers = entry => entry.next\nconst maxClockTimeReducer = (res, acc) => Math.max(res, acc.clock.time)\nconst uniqueEntriesReducer = (res, acc) => {\n  res[acc.hash] = acc\n  return res\n}\n\n/**\n * Log\n *\n * @description\n * Log implements a G-Set CRDT and adds ordering\n *\n * From:\n * \"A comprehensive study of Convergent and Commutative Replicated Data Types\"\n * https://hal.inria.fr/inria-00555588\n */\nclass Log extends GSet {\n  /**\n   * Create a new Log instance\n   * @param  {IPFS}           ipfs    An IPFS instance\n   * @param  {String}         id      ID of the log\n   * @param  {[Array<Entry>]} entries An Array of Entries from which to create the log from\n   * @param  {[Array<Entry>]} heads   Set the heads of the log\n   * @param  {[Clock]}        clock   Set the clock of the log\n   * @return {Log}            Log\n   */\n  constructor (ipfs, id, entries, heads, clock, key, keys = []) {\n    if (!isDefined(ipfs)) {\n      throw LogError.ImmutableDBNotDefinedError()\n    }\n\n    if (isDefined(entries) && !Array.isArray(entries)) {\n      throw new Error(`'entries' argument must be an array of Entry instances`)\n    }\n\n    if (isDefined(heads) && !Array.isArray(heads)) {\n      throw new Error(`'heads' argument must be an array`)\n    }\n\n    super()\n\n    this._storage = ipfs\n    this._id = id || randomId()\n\n    // Signing related setup\n    this._keystore = this._storage.keystore\n    this._key = key\n    this._keys = Array.isArray(keys) ? keys : [keys]\n\n    // Add entries to the internal cache\n    entries = entries || []\n    this._entryIndex = entries.reduce(uniqueEntriesReducer, {})\n\n    // Set heads if not passed as an argument\n    heads = heads || Log.findHeads(entries)\n    this._headsIndex = heads.reduce(uniqueEntriesReducer, {})\n\n    // Index of all next pointers in this log\n    this._nextsIndex = {}\n    entries.forEach(e => e.next.forEach(a => this._nextsIndex[a] = e.hash))\n\n    // Set the length, we calculate the length manually internally\n    this._length = entries ? entries.length : 0\n\n    // Set the clock\n    const maxTime = Math.max(clock ? clock.time : 0, this.heads.reduce(maxClockTimeReducer, 0))\n    this._clock = new Clock(this.id, maxTime)\n  }\n\n  /**\n   * Returns the ID of the log\n   * @returns {string}\n   */\n  get id () {\n    return this._id\n  }\n\n  /**\n   * Returns the clock of the log\n   * @returns {string}\n   */\n  get clock () {\n    return this._clock\n  }\n\n  /**\n   * Returns the length of the log\n   * @return {Number} Length\n   */\n  get length () {\n    return this._length\n  }\n\n  /**\n   * Returns the values in the log\n   * @returns {Array<Entry>}\n   */\n  get values () {\n    return Object.values(this._entryIndex).sort(Entry.compare) || []\n  }\n\n  /**\n   * Returns an array of heads as multihashes\n   * @returns {Array<string>}\n   */\n  get heads () {\n    return Object.values(this._headsIndex) || []\n  }\n\n  /**\n   * Returns an array of Entry objects that reference entries which\n   * are not in the log currently\n   * @returns {Array<Entry>}\n   */\n  get tails () {\n    return Log.findTails(this.values)\n  }\n\n  /**\n   * Returns an array of multihashes that are referenced by entries which\n   * are not in the log currently\n   * @returns {Array<string>} Array of multihashes\n   */\n  get tailHashes () {\n    return Log.findTailHashes(this.values)\n  }\n\n  /**\n   * Find an entry\n   * @param {string} [hash] The Multihash of the entry as Base58 encoded string\n   * @returns {Entry|undefined}\n   */\n  get (hash) {\n    return this._entryIndex[hash]\n  }\n\n  has (entry) {\n    return this._entryIndex[entry.hash || entry] !== undefined\n  }\n\n  traverse (rootEntries, amount) {\n    // console.log(\"traverse>\", rootEntry)\n    let stack = rootEntries.map(getNextPointers).reduce(flatMap, [])\n    let traversed = {}\n    let result = {}\n    let count = 0\n\n    const addToStack = hash => {\n      if (!result[hash] && !traversed[hash]) {\n        stack.push(hash)\n        traversed[hash] = true\n      }\n    }\n\n    const addRootHash = rootEntry => {\n      result[rootEntry.hash] = rootEntry.hash\n      traversed[rootEntry.hash] = true\n      count ++\n    }\n\n    rootEntries.forEach(addRootHash)\n\n    while (stack.length > 0 && count < amount) {\n      const hash = stack.shift()\n      const entry = this.get(hash)\n      if (entry) {\n        count ++\n        result[entry.hash] = entry.hash\n        traversed[entry.hash] = true\n        entry.next.forEach(addToStack)\n      }\n    }\n    return result\n  }\n\n  /**\n   * Append an entry to the log\n   * @param  {Entry} entry Entry to add\n   * @return {Log}   New Log containing the appended value\n   */\n  async append (data, pointerCount = 1) {\n    // Verify that we're allowed to append\n    if (this._key \n        && !this._keys.includes(this._key.getPublic('hex')) \n        && !this._keys.includes('*')) {\n      throw new Error(\"Not allowed to write\")\n    }\n\n    // Update the clock (find the latest clock)\n    const newTime = Math.max(this.clock.time, this.heads.reduce(maxClockTimeReducer, 0)) + 1\n    this._clock = new Clock(this.clock.id, newTime)\n    // Get the required amount of hashes to next entries (as per current state of the log)\n    const nexts = Object.keys(this.traverse(this.heads, pointerCount))\n    // Create the entry and add it to the internal cache\n    const entry = await Entry.create(this._storage, this.id, data, nexts, this.clock, this._key)\n    this._entryIndex[entry.hash] = entry\n    nexts.forEach(e => this._nextsIndex[e] = entry.hash)\n    this._headsIndex = {}\n    this._headsIndex[entry.hash] = entry\n    // Update the length\n    this._length ++\n    return entry\n  }\n\n  /**\n   * Join two logs\n   *\n   * @description Joins two logs returning a new log. Doesn't mutate the original logs.\n   *\n   * @param {IPFS}   [ipfs] An IPFS instance\n   * @param {Log}    log    Log to join with this Log\n   * @param {Number} [size] Max size of the joined log\n   * @param {string} [id]   ID to use for the new log\n   *\n   * @example\n   * log1.join(log2)\n   *\n   * @returns {Promise<Log>}\n   */\n  async join (log, size = -1, id) {\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!Log.isLog(log)) throw LogError.NotALogError()\n\n    // Verify the entries\n    // TODO: move to Entry\n    const verifyEntries = async (entries) => {\n      const isTrue = e => e === true\n      const getPubKey = e => e.getPublic ? e.getPublic('hex') : e\n      const checkAllKeys = (keys, entry) => {\n        const keyMatches = e => e === entry.key\n        return keys.find(keyMatches)\n      }\n      const pubkeys = this._keys.map(getPubKey)\n\n      const verify = async (entry) => {\n        if (!entry.key) throw new Error(\"Entry doesn't have a public key\")\n        if (!entry.sig) throw new Error(\"Entry doesn't have a signature\")\n\n        if (this._keys.length === 1 && this._keys[0] === this._key ) {\n          if (entry.id !== this.id) \n            throw new Error(\"Entry doesn't belong in this log (wrong ID)\")\n        }\n\n        if (this._keys.length > 0 \n            && !this._keys.includes('*') \n            && !checkAllKeys(this._keys.concat([this._key]), entry)) {\n          console.warn(\"Warning: Input log contains entries that are not allowed in this log. Logs weren't joined.\")\n          return false\n        }\n\n        try {\n          await Entry.verifyEntry(entry, this._keystore)\n        } catch (e) {\n          console.log(e)\n          console.log(\"Couldn't verify entry:\\n\", entry)\n          return false\n        }\n\n        return true\n      }\n\n      const checked = await pMap(entries, verify)\n      return checked.every(isTrue)\n    }\n\n    const difference = (log, exclude) => {\n      let stack = Object.keys(log._headsIndex)\n      let traversed = {}\n      let res = {}\n\n      const pushToStack = hash => {\n        if (!traversed[hash] && !exclude.get(hash)) {\n          stack.push(hash)\n          traversed[hash] = true\n        }\n      }\n\n      while (stack.length > 0) {\n        const hash = stack.shift()\n        const entry = log.get(hash)\n          if (entry && !exclude.get(hash)) {\n          res[entry.hash] = entry\n          traversed[entry.hash] = true\n          entry.next.forEach(pushToStack)\n        }\n      }\n      return res\n    }\n\n    // If id is not specified, use greater id of the two logs\n    id = id ? id : [log, this].sort((a, b) => a.id > b.id)[0].id\n\n    // Merge the entries\n    const newItems = difference(log, this)\n\n    // if a key was given, verify the entries from the incoming log\n    if (this._key) {\n      const canJoin = await verifyEntries(Object.values(newItems))\n      // Return early if any of the given entries didn't verify\n      if (!canJoin)\n        return this\n    }\n\n    // Update the internal entry index\n    this._entryIndex = Object.assign(this._entryIndex, newItems)\n\n    // Update the internal next pointers index\n    const addToNextsIndex = e => e.next.forEach(a => this._nextsIndex[a] = e.hash)\n    Object.values(newItems).forEach(addToNextsIndex)\n\n    // Update the length\n    this._length += Object.values(newItems).length\n\n    // Slice to the requested size\n    if (size > -1) {\n      let tmp = this.values\n      tmp = tmp.slice(-size)\n      this._entryIndex = tmp.reduce(uniqueEntriesReducer, {})\n      this._length = Object.values(this._entryIndex).length\n    }\n\n    // Merge the heads\n    const notReferencedByNewItems = e => !nextsFromNewItems.find(a => a === e.hash)\n    const notInCurrentNexts = e => !this._nextsIndex[e.hash]\n    const nextsFromNewItems = Object.values(newItems).map(getNextPointers).reduce(flatMap, [])\n    const mergedHeads = Log.findHeads(Object.values(Object.assign({}, this._headsIndex, log._headsIndex)))\n      .filter(notReferencedByNewItems)\n      .filter(notInCurrentNexts)\n      .reduce(uniqueEntriesReducer, {})\n\n    this._headsIndex = mergedHeads\n\n    // Find the latest clock from the heads\n    const maxClock = Object.values(this._headsIndex).reduce(maxClockTimeReducer, 0)\n    const clock = new Clock(this.id, Math.max(this.clock.time, maxClock))\n\n    this._id = id\n    this._clock = clock\n    return this\n  }\n\n  /**\n   * Get the log in JSON format\n   * @returns {Object<{heads}>}\n   */\n  toJSON () {\n    return {\n      id: this.id,\n      heads: this.heads.map(getHash)\n    }\n  }\n\n  toSnapshot () {\n    return {\n      id: this.id,\n      heads: this.heads,\n      values: this.values,\n    }\n  }\n  /**\n   * Get the log as a Buffer\n   * @returns {Buffer}\n   */\n  toBuffer () {\n    return Buffer.from(JSON.stringify(this.toJSON()))\n  }\n\n  /**\n   * Returns the log entries as a formatted string\n   * @example\n   * two\n   * one\n   *   three\n   * @returns {string}\n   */\n  toString (payloadMapper) {\n    return this.values\n      .slice()\n      .reverse()\n      .map((e, idx) => {\n        const parents = Entry.findChildren(e, this.values)\n        const len = parents.length\n        let padding = new Array(Math.max(len - 1, 0))\n        padding = len > 1 ? padding.fill('  ') : padding\n        padding = len > 0 ? padding.concat(['']) : padding\n        return padding.join('') + (payloadMapper ? payloadMapper(e.payload) : e.payload)\n      })\n      .join('\\n')\n  }\n\n  /**\n   * Check whether an object is a Log instance\n   * @param {Object} log An object to check\n   * @returns {true|false}\n   */\n  static isLog (log) {\n    return log.id !== undefined\n      && log.heads !== undefined\n      && log._entryIndex !== undefined\n  }\n\n  /**\n   * Get the log's multihash\n   * @returns {Promise<string>} Multihash of the Log as Base58 encoded string\n   */\n  toMultihash () {\n    return LogIO.toMultihash(this._storage, this)\n  }\n\n  /**\n   * Create a log from multihash\n   * @param {IPFS}   ipfs        An IPFS instance\n   * @param {string} hash        Multihash (as a Base58 encoded string) to create the log from\n   * @param {Number} [length=-1] How many items to include in the log\n   * @param {Function(hash, entry, parent, depth)} onProgressCallback\n   * @return {Promise<Log>}      New Log\n   */\n  static fromMultihash (ipfs, hash, length = -1, exclude, key, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`)\n\n    // TODO: need to verify the entries with 'key'\n    return LogIO.fromMultihash(ipfs, hash, length, exclude, onProgressCallback)\n      .then((data) => new Log(ipfs, data.id, data.values, data.heads, data.clock, key))\n  }\n\n  /**\n   * Create a log from a single entry's multihash\n   * @param {IPFS}   ipfs        An IPFS instance\n   * @param {string} hash        Multihash (as a Base58 encoded string) of the Entry from which to create the log from\n   * @param {Number} [length=-1] How many entries to include in the log\n   * @param {Function(hash, entry, parent, depth)} onProgressCallback\n   * @return {Promise<Log>}      New Log\n   */\n  static fromEntryHash (ipfs, hash, id, length = -1, exclude, key, keys, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(hash)) throw new Error(\"'hash' must be defined\")\n\n    // TODO: need to verify the entries with 'key'\n    return LogIO.fromEntryHash(ipfs, hash, id, length, exclude, onProgressCallback)\n      .then((data) => new Log(ipfs, id, data.values, null, null, key, keys))\n  }\n\n  /**\n   * Create a log from a Log Snapshot JSON\n   * @param {IPFS} ipfs          An IPFS instance\n   * @param {Object} json        Log snapshot as JSON object\n   * @param {Number} [length=-1] How many entries to include in the log\n   * @param {Function(hash, entry, parent, depth)} [onProgressCallback]\n   * @return {Promise<Log>}      New Log\n   */\n  static fromJSON (ipfs, json, length = -1, key, keys, timeout, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n\n    // TODO: need to verify the entries with 'key'\n    return LogIO.fromJSON(ipfs, json, length, key, timeout, onProgressCallback)\n      .then((data) => new Log(ipfs, data.id, data.values, null, null, key, keys))\n  }\n\n  /**\n   * Create a new log from an Entry instance\n   * @param {IPFS}                ipfs          An IPFS instance\n   * @param {Entry|Array<Entry>}  sourceEntries An Entry or an array of entries to fetch a log from\n   * @param {Number}              [length=-1]   How many entries to include. Default: infinite.\n   * @param {Array<Entry|string>} [exclude]     Array of entries or hashes or entries to not fetch (foe eg. cached entries)\n   * @param {Function(hash, entry, parent, depth)} [onProgressCallback]\n   * @return {Promise<Log>}       New Log\n   */\n  static fromEntry (ipfs, sourceEntries, length = -1, exclude, onProgressCallback) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\")\n\n    // TODO: need to verify the entries with 'key'\n    return LogIO.fromEntry(ipfs, sourceEntries, length, exclude, onProgressCallback)\n      .then((data) => new Log(ipfs, data.id, data.values))\n  }\n\n  /**\n   * Expands the log with a specified number of new values\n   *\n   * @param  {IPFS}               ipfs    An IPFS instance\n   * @param  {Log}                log     Log to expand\n   * @param  {Entry|Array<Entry>} entries An Entry or an Array of entries to expand from\n   * @param  {Number}             amount  How many new entries to include\n   * @return {Promise<Log>}       New Log\n   */\n  static expandFrom (ipfs, log, entries, amount = -1) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!isDefined(entries)) throw new Error(`'entries' must be given as argument`)\n    if (!Log.isLog(log)) throw LogError.NotALogError()\n\n    return LogIO.expandFrom(ipfs, log, entries, amount)\n      .then((data) => new Log(ipfs, log.id, data.values, null, log.clock))\n  }\n\n  /**\n   * Expands the log with a specified amount of Entries\n   * @param  {IPFS}   ipfs   An IPFS instance\n   * @param  {Log}    log    Log to expand\n   * @param  {Number} amount How many new entries to include\n   * @return {Promise<Log>}  New Log\n   */\n  static expand (ipfs, log, amount) {\n    if (!isDefined(ipfs)) throw LogError.ImmutableDBNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!Log.isLog(log)) throw LogError.NotALogError()\n\n    return LogIO.expand(ipfs, log, amount)\n      .then((data) => new Log(ipfs, log.id, data.values, log.heads, log.clock))\n  }\n\n  /**\n   * Find heads from a collection of entries\n   *\n   * @description\n   * Finds entries that are the heads of this collection,\n   * ie. entries that are not referenced by other entries\n   *\n   * @param {Array<Entry>} Entries to search heads from\n   * @returns {Array<Entry>}\n   */\n  static findHeads (entries) {\n    var indexReducer = (res, entry, idx, arr) => {\n      var addToResult = e => res[e] = entry.hash\n      entry.next.forEach(addToResult)\n      return res\n    }\n\n    var items = entries.reduce(indexReducer, {})\n\n    var exists = e => items[e.hash] === undefined\n    var compareIds = (a, b) => a.id > b.id\n\n    return entries.filter(exists).sort(compareIds)\n  }\n\n  // Find entries that point to another entry that is not in the\n  // input array\n  static findTails (entries) {\n    // Reverse index { next -> entry }\n    var reverseIndex = {}\n    // Null index containing entries that have no parents (nexts)\n    var nullIndex = []\n    // Hashes for all entries for quick lookups\n    var hashes = {}\n    // Hashes of all next entries\n    var nexts = []\n\n    var addToIndex = (e) => {\n      if (e.next.length === 0) {\n        nullIndex.push(e)\n      }\n      var addToReverseIndex = (a) => {\n        /* istanbul ignore else */\n        if (!reverseIndex[a]) reverseIndex[a] = []\n        reverseIndex[a].push(e)\n      }\n\n      // Add all entries and their parents to the reverse index\n      e.next.forEach(addToReverseIndex)\n      // Get all next references\n      nexts = nexts.concat(e.next)\n      // Get the hashes of input entries\n      hashes[e.hash] = true\n    }\n\n    // Create our indices\n    entries.forEach(addToIndex)\n\n    var addUniques = (res, entries, idx, arr) => res.concat(_uniques(entries, 'hash'))\n    var exists = e => hashes[e] === undefined\n    var findFromReverseIndex = e => reverseIndex[e]\n\n    // Drop hashes that are not in the input entries\n    const tails = nexts // For every multihash in nexts:\n      .filter(exists) // Remove undefineds and nulls\n      .map(findFromReverseIndex) // Get the Entry from the reverse index\n      .reduce(addUniques, []) // Flatten the result and take only uniques\n      .concat(nullIndex) // Combine with tails the have no next refs (ie. first-in-their-chain)\n\n    return _uniques(tails, 'hash').sort(Entry.compare)\n  }\n\n  // Find the hashes to entries that are not in a collection\n  // but referenced by other entries\n  static findTailHashes (entries) {\n    var hashes = {}\n    var addToIndex = (e) => hashes[e.hash] = true\n\n    var reduceTailHashes = (res, entry, idx, arr) => {\n      var addToResult = (e) => {\n        /* istanbul ignore else */\n        if (hashes[e] === undefined) {\n          res.splice(0, 0, e)\n        }\n      }\n      entry.next.reverse().forEach(addToResult)\n      return res\n    }\n\n    entries.forEach(addToIndex)\n    return entries.reduce(reduceTailHashes, [])\n  }\n}\n\nmodule.exports = Log\n"],"sourceRoot":""}